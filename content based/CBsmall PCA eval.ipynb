{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68b063617afebd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**1) Build Article Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41dabf98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:05:19.289137700Z",
     "start_time": "2025-04-10T22:05:17.671279900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:05:17] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 00:05:17] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 00:05:17] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bex\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 00:05:19] We saw that you have a AMD Ryzen 7 7800X3D 8-Core Processor but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 00:05:19] CPU Model on constant consumption mode: AMD Ryzen 7 7800X3D 8-Core Processor\n",
      "[codecarbon INFO @ 00:05:19] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 00:05:19] No GPU found.\n",
      "[codecarbon INFO @ 00:05:19] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 00:05:19]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 00:05:19]   Python version: 3.12.4\n",
      "[codecarbon INFO @ 00:05:19]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 00:05:19]   Available RAM : 63.213 GB\n",
      "[codecarbon INFO @ 00:05:19]   CPU count: 16\n",
      "[codecarbon INFO @ 00:05:19]   CPU model: AMD Ryzen 7 7800X3D 8-Core Processor\n",
      "[codecarbon INFO @ 00:05:19]   GPU count: None\n",
      "[codecarbon INFO @ 00:05:19]   GPU model: None\n",
      "[codecarbon INFO @ 00:05:19] Saving emissions data to file C:\\Users\\Bex\\OneDrive - NTNU\\NTNU\\4 Ã¥r\\Recommender systems\\Project\\content based\\emissions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"def dcg_at_k(recommended, actual_clicked, K=5):\\n    return sum(1 / np.log2(i + 2) for i, item in enumerate(recommended[:K]) if item in actual_clicked)\\n\\ndef idcg_at_k(actual_clicked, K=5):\\n    num_relevant = min(len(actual_clicked), K)\\n    return sum(1 / np.log2(i + 2) for i in range(num_relevant)) or 1\\n\\ndef ndcg_at_k(recommended, actual_clicked, K=5):\\n    return dcg_at_k(recommended, actual_clicked, K) / idcg_at_k(actual_clicked, K)\\n\\ndef auc_at_k(recommended, actual_clicked, K=5):\\n    recommended = recommended[:K]\\n    relevance = [1 if item in actual_clicked else 0 for item in recommended]\\n    num_pos = sum(relevance)\\n    num_neg = len(relevance) - num_pos\\n    if num_pos == 0: return 0.0\\n    if num_neg == 0: return 1.0\\n    correct_pairs = sum(1 for i in range(len(relevance)) if relevance[i] == 1\\n                        for j in range(i+1, len(relevance)) if relevance[j] == 0)\\n    return correct_pairs / (num_pos * num_neg)\\n\\ndef mrr_at_k(recommended, actual_clicked, K=5):\\n    recommended = recommended[:K]\\n    for i, item in enumerate(recommended):\\n        if item in actual_clicked:\\n            return 1.0 / (i + 1)\\n    return 0.0\\n\\ndef evaluate_model_self(user_profiles, cluster_to_articles, train_behaviors_df,\\n                   test_behaviors_df, article_vectors, pca, K=5):\\n    ndcg_scores, mrr_scores, auc_scores = [], [], []\\n    for _, row in test_behaviors_df.iterrows():\\n        user_id = row['user_id']\\n        if pd.isna(row['impressions']):\\n            continue\\n        all_ids, clicked = parse_impressions(row['impressions'])\\n        if not clicked:\\n            continue\\n\\n        recommended = recommend_for_user(user_id, user_profiles, cluster_to_articles,\\n                                         train_behaviors_df, article_vectors, pca,\\n                                         top_clusters=5, top_k=len(all_ids))\\n        ndcg_scores.append(ndcg_at_k(recommended, clicked, K))\\n        mrr_scores.append(mrr_at_k(recommended, clicked, K))\\n        auc_scores.append(auc_at_k(recommended, clicked, K))\\n\\n    return np.mean(ndcg_scores), np.mean(auc_scores), np.mean(mrr_scores)\\n\\n    \""
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "from recommenders.utils.timer import Timer\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "\n",
    "\n",
    "def filter_users_with_min_clicks(df, min_clicks=5):\n",
    "    df['click_count'] = df['history'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "    filtered_users = df[df['click_count'] >= min_clicks].drop(columns=['click_count'])\n",
    "    print(len(filtered_users))\n",
    "    return df[df['click_count'] >= min_clicks].drop(columns=['click_count'])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Load and Preprocess Article Embeddings\n",
    "# ----------------------------------------------------\n",
    "def parse_entity_list(entity_str):\n",
    "    try:\n",
    "        data = json.loads(entity_str)\n",
    "        return [obj['WikidataId'] for obj in data if 'WikidataId' in obj]\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "def build_article_embeddings(news_df, entity_embeddings, use_abstract=True):\n",
    "    article_vectors = {}\n",
    "    for news_id, row in news_df.iterrows():\n",
    "        title_ids = parse_entity_list(row['title_entities'])\n",
    "        abstract_ids = parse_entity_list(row['abstract_entities']) if use_abstract else []\n",
    "        entity_ids = title_ids + abstract_ids\n",
    "        vectors = [entity_embeddings[eid] for eid in entity_ids if eid in entity_embeddings]\n",
    "        if vectors:\n",
    "            article_vectors[news_id] = np.mean(vectors, axis=0)\n",
    "    return article_vectors\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2: Fit PCA on Article Embeddings\n",
    "# ----------------------------------------------------\n",
    "def apply_pca_to_article_vectors(article_vectors, n_components=50):\n",
    "    article_ids = list(article_vectors.keys())\n",
    "    matrix = np.stack([article_vectors[aid] for aid in article_ids])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_matrix = pca.fit_transform(matrix)\n",
    "    reduced_article_vectors = {aid: reduced_matrix[i] for i, aid in enumerate(article_ids)}\n",
    "    return reduced_article_vectors, pca\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3: Build User Profiles (from article embeddings)\n",
    "# ----------------------------------------------------\n",
    "def build_user_profiles_embedding_based(behaviors_df, article_vectors):\n",
    "    user_profiles = {}\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        if not isinstance(row['history'], str):\n",
    "            continue\n",
    "        clicked_ids = row['history'].split()\n",
    "        vectors = [article_vectors[aid] for aid in clicked_ids if aid in article_vectors]\n",
    "        if vectors:\n",
    "            user_profiles[user_id] = np.mean(vectors, axis=0)\n",
    "    return user_profiles\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 4: Recommender Function Using PCA\n",
    "# ----------------------------------------------------\n",
    "def recommend_for_user(user_id, user_profiles, article_vectors, candidate_articles=None, top_k=5):\n",
    "    if user_id not in user_profiles:\n",
    "        return []\n",
    "\n",
    "    user_embedding = user_profiles[user_id]\n",
    "\n",
    "    # Filter candidates\n",
    "    if candidate_articles is None:\n",
    "        candidate_articles = list(article_vectors.keys())\n",
    "    else:\n",
    "        candidate_articles = [aid for aid in candidate_articles if aid in article_vectors]\n",
    "\n",
    "    if not candidate_articles:\n",
    "        return []\n",
    "\n",
    "    scored = []\n",
    "    for aid in candidate_articles:\n",
    "        art_vec = article_vectors[aid]\n",
    "        sim = np.dot(user_embedding, art_vec) / (norm(user_embedding) * norm(art_vec))\n",
    "        scored.append((aid, sim))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in scored[:top_k]]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 5: Evaluation Functions\n",
    "# ----------------------------------------------------\n",
    "def parse_impressions(impressions_str):\n",
    "    items = impressions_str.split()\n",
    "    all_ids = []\n",
    "    clicked = []\n",
    "    for x in items:\n",
    "        article_id, label_str = x.split('-')\n",
    "        all_ids.append(article_id)\n",
    "        if label_str == '1':\n",
    "            clicked.append(article_id)\n",
    "    return all_ids, clicked\n",
    "\n",
    "\"\"\"def dcg_at_k(recommended, actual_clicked, K=5):\n",
    "    return sum(1 / np.log2(i + 2) for i, item in enumerate(recommended[:K]) if item in actual_clicked)\n",
    "\n",
    "def idcg_at_k(actual_clicked, K=5):\n",
    "    num_relevant = min(len(actual_clicked), K)\n",
    "    return sum(1 / np.log2(i + 2) for i in range(num_relevant)) or 1\n",
    "\n",
    "def ndcg_at_k(recommended, actual_clicked, K=5):\n",
    "    return dcg_at_k(recommended, actual_clicked, K) / idcg_at_k(actual_clicked, K)\n",
    "\n",
    "def auc_at_k(recommended, actual_clicked, K=5):\n",
    "    recommended = recommended[:K]\n",
    "    relevance = [1 if item in actual_clicked else 0 for item in recommended]\n",
    "    num_pos = sum(relevance)\n",
    "    num_neg = len(relevance) - num_pos\n",
    "    if num_pos == 0: return 0.0\n",
    "    if num_neg == 0: return 1.0\n",
    "    correct_pairs = sum(1 for i in range(len(relevance)) if relevance[i] == 1\n",
    "                        for j in range(i+1, len(relevance)) if relevance[j] == 0)\n",
    "    return correct_pairs / (num_pos * num_neg)\n",
    "\n",
    "def mrr_at_k(recommended, actual_clicked, K=5):\n",
    "    recommended = recommended[:K]\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in actual_clicked:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_model_self(user_profiles, cluster_to_articles, train_behaviors_df,\n",
    "                   test_behaviors_df, article_vectors, pca, K=5):\n",
    "    ndcg_scores, mrr_scores, auc_scores = [], [], []\n",
    "    for _, row in test_behaviors_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "        all_ids, clicked = parse_impressions(row['impressions'])\n",
    "        if not clicked:\n",
    "            continue\n",
    "\n",
    "        recommended = recommend_for_user(user_id, user_profiles, cluster_to_articles,\n",
    "                                         train_behaviors_df, article_vectors, pca,\n",
    "                                         top_clusters=5, top_k=len(all_ids))\n",
    "        ndcg_scores.append(ndcg_at_k(recommended, clicked, K))\n",
    "        mrr_scores.append(mrr_at_k(recommended, clicked, K))\n",
    "        auc_scores.append(auc_at_k(recommended, clicked, K))\n",
    "\n",
    "    return np.mean(ndcg_scores), np.mean(auc_scores), np.mean(mrr_scores)\n",
    "\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2319f29b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:05:29.201687700Z",
     "start_time": "2025-04-10T22:05:19.293139200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153727\n",
      "70938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:05:22] Energy consumed for RAM : 0.000010 kWh. RAM Power : 23.70497703552246 W\n",
      "[codecarbon INFO @ 00:05:22] Energy consumed for all CPUs : 0.000018 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:05:22] 0.000028 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per article (embedding + PCA): 0.000029 seconds\n",
      "Total energy for embedding + PCA: 0.000001 kWh\n",
      "Energy per user profile: 8.352661947815002e-07 /, 51282.00000000 = 0.00000000 kWh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 00:05:24] Already started tracking\n",
      "[codecarbon WARNING @ 00:05:29] Tracker already stopped !\n",
      "[codecarbon INFO @ 00:05:29] Energy consumed for RAM : 0.000051 kWh. RAM Power : 23.70497703552246 W\n",
      "[codecarbon INFO @ 00:05:29] Energy consumed for all CPUs : 0.000091 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:05:29] 0.000142 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per user profile: 0.000102 seconds\n",
      "Total energy consumed: 0.000004 kWh\n",
      "Energy per user profile: 4.260266339042304e-06 /, 49108.00000000 = 0.00000000 kWh\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n# ----- Step 5: Evaluate -----\\navg_ndcg, avg_auc, avg_mrr = evaluate_model_self(\\n    user_profiles=user_profiles,\\n    cluster_to_articles=cluster_to_articles,\\n    train_behaviors_df=train_behaviors_df,\\n    test_behaviors_df=test_behaviors_df,\\n    article_vectors=reduced_article_vectors,\\n    pca=pca,\\n    K=5\\n)\\n\\n# ----- Print Results -----\\nprint(f\"Evaluation Results:\")\\nprint(f\"NDCG@5: {avg_ndcg:.4f}\")\\nprint(f\"AUC@5:  {avg_auc:.4f}\")\\nprint(f\"MRR@5:  {avg_mrr:.4f}\")\\n'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "# ----- File Paths -----\n",
    "dataset_dir = \"../MINDsmall_train\"\n",
    "news_file = os.path.join(dataset_dir, \"news.tsv\")\n",
    "embedding_file = os.path.join(dataset_dir, \"entity_embedding.vec\")\n",
    "train_behaviors_file = os.path.join(dataset_dir, \"behaviors.tsv\")\n",
    "test_behaviors_file = \"../MINDsmall_dev/behaviors.tsv\"\n",
    "\n",
    "# ----- Load data -----\n",
    "news_cols = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "news_df = pd.read_csv(news_file, sep='\\t', header=None, names=news_cols)\n",
    "news_df.set_index('news_id', inplace=True)\n",
    "\n",
    "train_behaviors_df = pd.read_csv(train_behaviors_file, sep='\\t', header=None,\n",
    "    names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "train_behaviors_df = filter_users_with_min_clicks(train_behaviors_df, min_clicks=1)\n",
    "\n",
    "test_behaviors_df = pd.read_csv(test_behaviors_file, sep='\\t', header=None,\n",
    "    names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "test_behaviors_df = filter_users_with_min_clicks(test_behaviors_df, min_clicks=1)\n",
    "\n",
    "# Filter test behaviors to only include users present in train behaviors\n",
    "valid_users = set(train_behaviors_df['user_id'].unique())\n",
    "test_behaviors_df = test_behaviors_df[test_behaviors_df['user_id'].isin(valid_users)]\n",
    "\n",
    "\n",
    "# ----- Load entity embeddings -----\n",
    "entity_embeddings = {}\n",
    "with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        entity_id = parts[0]\n",
    "        vector = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
    "        entity_embeddings[entity_id] = vector\n",
    "\n",
    "\n",
    "\n",
    "# ----- Step 1: Build article embeddings -----\n",
    "tracker.start()\n",
    "with Timer() as train_time:\n",
    "    \n",
    "    article_vectors = build_article_embeddings(news_df, entity_embeddings, use_abstract=True)\n",
    "\n",
    "    # ----- Step 2: Apply PCA -----\n",
    "    reduced_article_vectors, pca = apply_pca_to_article_vectors(article_vectors, n_components=50)\n",
    "\n",
    " \n",
    "rows = news_df.shape[0]\n",
    "print(f\"Time per article (embedding + PCA): {train_time.interval / rows:.6f} seconds\")\n",
    "total_kwh = tracker.stop()\n",
    "\n",
    "\n",
    "print(f\"Total energy for embedding + PCA: {total_kwh:.6f} kWh\")\n",
    "print(f\"Energy per user profile: {total_kwh} /, {rows:.8f} = {total_kwh / rows:.8f} kWh\") \n",
    "\n",
    "# ----- Step 3: Cluster articles (e.g. using KMeans) -----\n",
    "from sklearn.cluster import KMeans\n",
    "X = np.stack(list(reduced_article_vectors.values()))\n",
    "article_ids = list(reduced_article_vectors.keys())\n",
    "kmeans = KMeans(n_clusters=20, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "cluster_assignments = {aid: int(label) for aid, label in zip(article_ids, kmeans.labels_)}\n",
    "cluster_to_articles = defaultdict(list)\n",
    "for aid, label in cluster_assignments.items():\n",
    "    cluster_to_articles[label].append(aid)\n",
    "\n",
    "# ----- Step 4: Build user cluster profiles -----\n",
    "def build_cluster_distribution_profiles(behaviors_df, cluster_assignments, num_clusters=50):\n",
    "    user_profiles = defaultdict(lambda: np.zeros(num_clusters, dtype=np.float32))\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        if not isinstance(row['history'], str):\n",
    "            continue\n",
    "        for aid in row['history'].split():\n",
    "            if aid in cluster_assignments:\n",
    "                user_profiles[row['user_id']][cluster_assignments[aid]] += 1\n",
    "    return {u: v / (v.sum() or 1.0) for u, v in user_profiles.items()}\n",
    "\n",
    "\n",
    "\n",
    "rows = train_behaviors_df['user_id'].nunique()\n",
    "\n",
    "tracker.start()\n",
    "with Timer() as profile_time:\n",
    "    user_profiles = build_user_profiles_embedding_based(train_behaviors_df, reduced_article_vectors)\n",
    "    \n",
    "print(f\"Time per user profile: {profile_time.interval / rows:.6f} seconds\")\n",
    "total_kwh = tracker.stop()\n",
    "\n",
    "\n",
    "print(f\"Total energy consumed: {total_kwh:.6f} kWh\")\n",
    "print(f\"Energy per user profile: {total_kwh} /, {rows:.8f} = {total_kwh / rows:.8f} kWh\")    \n",
    "\n",
    "\"\"\"\n",
    "# ----- Step 5: Evaluate -----\n",
    "avg_ndcg, avg_auc, avg_mrr = evaluate_model_self(\n",
    "    user_profiles=user_profiles,\n",
    "    cluster_to_articles=cluster_to_articles,\n",
    "    train_behaviors_df=train_behaviors_df,\n",
    "    test_behaviors_df=test_behaviors_df,\n",
    "    article_vectors=reduced_article_vectors,\n",
    "    pca=pca,\n",
    "    K=5\n",
    ")\n",
    "\n",
    "# ----- Print Results -----\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"NDCG@5: {avg_ndcg:.4f}\")\n",
    "print(f\"AUC@5:  {avg_auc:.4f}\")\n",
    "print(f\"MRR@5:  {avg_mrr:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62205370",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-04-10T22:05:29.201687700Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.evaluation import evaluate_model\n",
    "\n",
    "class ClusteredContentRecommender:\n",
    "    def __init__(self, user_profiles, article_vectors):\n",
    "        self.user_profiles = user_profiles\n",
    "        self.article_vectors = article_vectors\n",
    "\n",
    "    def recommend(self, user_id, candidate_articles=None, N=5):\n",
    "        # If candidate articles are passed (during evaluation), only use those.\n",
    "        if candidate_articles is not None:\n",
    "            filtered_articles = {aid: vec for aid, vec in self.article_vectors.items() if aid in candidate_articles}\n",
    "        else:\n",
    "            filtered_articles = self.article_vectors\n",
    "    \n",
    "        return recommend_for_user(\n",
    "            user_id=user_id,\n",
    "            user_profiles=self.user_profiles,\n",
    "            article_vectors=filtered_articles,\n",
    "            candidate_articles=list(filtered_articles.keys()),  # ensure it's passed\n",
    "            top_k=N\n",
    "        )\n",
    "\n",
    "\n",
    "# Instantiate the recommender\n",
    "recommender = ClusteredContentRecommender(\n",
    "    user_profiles=user_profiles,\n",
    "    article_vectors=reduced_article_vectors\n",
    ")\n",
    "rows = test_behaviors_df.shape[0]\n",
    "\n",
    "# Evaluate the model\n",
    "with Timer() as eval_time:\n",
    "    ndcg, auc, mrr = evaluate_model(recommender, test_behaviors_df, K=5)\n",
    "\n",
    "print(\"\\n **Evaluation Results:**\")\n",
    "print(f\" NDCG@5: {ndcg:.4f}\")\n",
    "print(f\" AUC@5:  {auc:.4f}\")\n",
    "print(f\" MRR@5:  {mrr:.4f}\")\n",
    "print(f\"Time per user impression: {eval_time.interval / rows:.6f} seconds\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
