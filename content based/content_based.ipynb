{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Recommenders contributors.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MIND Utils Generation\n",
    "\n",
    "MIND dataset\\[1\\] is a large-scale English news dataset. It was collected from anonymized behavior logs of Microsoft News website. MIND contains 1,000,000 users, 161,013 news articles and 15,777,377 impression logs. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression.\n",
    "\n",
    "Many news recommendation methods use word embeddings, news vertical embeddings, news subvertical embeddings and user id embedding. Therefore, it is necessary to generate a word dictionary, a vertical dictionary, a subvertical dictionary and a `userid` dictionary to convert words, news verticals, subverticals and user ids from strings to indexes. To use the pretrain word embedding, an embedding matrix is generated as the initial weight of the word embedding layer.\n",
    "\n",
    "This notebook gives examples about how to generate:\n",
    "* `word_dict.pkl`: convert the words in news titles into indexes.\n",
    "* `word_dict_all.pkl`: convert the words in news titles and abstracts into indexes.\n",
    "* `embedding.npy`: pretrained word embedding matrix of words in word_dict.pkl\n",
    "* `embedding_all.npy`: pretrained embedding matrix of words in word_dict_all.pkl\n",
    "* `vert_dict.pkl`: convert news verticals into indexes.\n",
    "* `subvert_dict.pkl`: convert news subverticals into indexes.\n",
    "* `uid2index.pkl`: convert user ids into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:06.846214300Z",
     "start_time": "2025-03-10T11:43:05.473338400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from recommenders.datasets.mind import (download_mind,\n",
    "                                     extract_mind,\n",
    "                                     download_and_extract_glove,\n",
    "                                     load_glove_matrix,\n",
    "                                     word_tokenize\n",
    "                                    )\n",
    "from recommenders.datasets.download_utils import unzip_file\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:06.849496200Z",
     "start_time": "2025-03-10T11:43:06.847245300Z"
    }
   },
   "outputs": [],
   "source": [
    "# MIND sizes: \"demo\", \"small\" or \"large\"\n",
    "mind_type=\"small\" \n",
    "# word_embedding_dim should be in [50, 100, 200, 300]\n",
    "word_embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:19.026717100Z",
     "start_time": "2025-03-10T11:43:06.849496200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51.8k/51.8k [00:06<00:00, 8.62kKB/s]\n",
      "100%|██████████| 30.2k/30.2k [00:04<00:00, 6.91kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "train_zip, valid_zip = download_mind(size=mind_type, dest_path=data_path)\n",
    "unzip_file(train_zip, os.path.join(data_path, 'train'), clean_zip_file=False)\n",
    "unzip_file(valid_zip, os.path.join(data_path, 'valid'), clean_zip_file=False)\n",
    "output_path = os.path.join(data_path, 'utils')\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare utils of news\n",
    "\n",
    "* word dictionary\n",
    "* vertical dictionary\n",
    "* subvetical dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:19.272058400Z",
     "start_time": "2025-03-10T11:43:19.026717100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_table(os.path.join(data_path, 'train', 'news.tsv'),\n",
    "                     names=['newid', 'vertical', 'subvertical', 'title',\n",
    "                            'abstract', 'url', 'entities in title', 'entities in abstract'],\n",
    "                     usecols = ['newid','vertical', 'subvertical', 'title', 'abstract'])\n",
    "\n",
    "print(len(news))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:19.287861200Z",
     "start_time": "2025-03-10T11:43:19.272058400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    newid   vertical      subvertical  \\\n0  N55528  lifestyle  lifestyleroyals   \n1  N19639     health       weightloss   \n2  N61837       news        newsworld   \n3  N53526     health           voices   \n4  N38324     health          medical   \n\n                                               title  \\\n0  The Brands Queen Elizabeth, Prince Charles, an...   \n1                      50 Worst Habits For Belly Fat   \n2  The Cost of Trump's Aid Freeze in the Trenches...   \n3  I Was An NBA Wife. Here's How It Affected My M...   \n4  How to Get Rid of Skin Tags, According to a De...   \n\n                                            abstract  \n0  Shop the notebooks, jackets, and more that the...  \n1  These seemingly harmless habits are holding yo...  \n2  Lt. Ivan Molchanets peeked over a parapet of s...  \n3  I felt like I was a fraud, and being an NBA wi...  \n4  They seem harmless, but there's a very good re...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>newid</th>\n      <th>vertical</th>\n      <th>subvertical</th>\n      <th>title</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>N55528</td>\n      <td>lifestyle</td>\n      <td>lifestyleroyals</td>\n      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n      <td>Shop the notebooks, jackets, and more that the...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>N19639</td>\n      <td>health</td>\n      <td>weightloss</td>\n      <td>50 Worst Habits For Belly Fat</td>\n      <td>These seemingly harmless habits are holding yo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>N61837</td>\n      <td>news</td>\n      <td>newsworld</td>\n      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>N53526</td>\n      <td>health</td>\n      <td>voices</td>\n      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n      <td>I felt like I was a fraud, and being an NBA wi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>N38324</td>\n      <td>health</td>\n      <td>medical</td>\n      <td>How to Get Rid of Skin Tags, According to a De...</td>\n      <td>They seem harmless, but there's a very good re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:19.289047900Z",
     "start_time": "2025-03-10T11:43:19.283647200Z"
    }
   },
   "outputs": [],
   "source": [
    "news_vertical = news.vertical.drop_duplicates().reset_index(drop=True)\n",
    "vert_dict_inv = news_vertical.to_dict()\n",
    "vert_dict = {v: k+1 for k, v in vert_dict_inv.items()}\n",
    "\n",
    "news_subvertical = news.subvertical.drop_duplicates().reset_index(drop=True)\n",
    "subvert_dict_inv = news_subvertical.to_dict()\n",
    "subvert_dict = {v: k+1 for k, v in vert_dict_inv.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:20.022206100Z",
     "start_time": "2025-03-10T11:43:19.289047900Z"
    }
   },
   "outputs": [],
   "source": [
    "news.title = news.title.apply(word_tokenize)\n",
    "news.abstract = news.abstract.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:21.967649100Z",
     "start_time": "2025-03-10T11:43:20.018198700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51282/51282 [00:01<00:00, 26401.93it/s]\n"
     ]
    }
   ],
   "source": [
    "word_cnt = Counter()\n",
    "word_cnt_all = Counter()\n",
    "\n",
    "for i in tqdm(range(len(news))):\n",
    "    word_cnt.update(news.loc[i]['title'])\n",
    "    word_cnt_all.update(news.loc[i]['title'])\n",
    "    word_cnt_all.update(news.loc[i]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:21.980757500Z",
     "start_time": "2025-03-10T11:43:21.966643700Z"
    }
   },
   "outputs": [],
   "source": [
    "word_dict = {k: v+1 for k, v in zip(word_cnt, range(len(word_cnt)))}\n",
    "word_dict_all = {k: v+1 for k, v in zip(word_cnt_all, range(len(word_cnt_all)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:21.999118400Z",
     "start_time": "2025-03-10T11:43:21.982266100Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'vert_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(vert_dict, f)\n",
    "    \n",
    "with open(os.path.join(output_path, 'subvert_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(subvert_dict, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'word_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_dict, f)\n",
    "    \n",
    "with open(os.path.join(output_path, 'word_dict_all.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_dict_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embedding matrixs\n",
    "* embedding.npy\n",
    "* embedding_all.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:37.793711900Z",
     "start_time": "2025-03-10T11:43:21.999118400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 842k/842k [00:09<00:00, 90.1kKB/s] \n"
     ]
    }
   ],
   "source": [
    "glove_path = download_and_extract_glove(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:47.239516500Z",
     "start_time": "2025-03-10T11:43:37.795721100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:04, 91479.56it/s] \n",
      "400001it [00:05, 79081.33it/s] \n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, exist_word = load_glove_matrix(glove_path, word_dict, word_embedding_dim)\n",
    "embedding_all_matrix, exist_all_word = load_glove_matrix(glove_path, word_dict_all, word_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:47.368034400Z",
     "start_time": "2025-03-10T11:43:47.237770800Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_path, 'embedding.npy'), embedding_matrix)\n",
    "np.save(os.path.join(output_path, 'embedding_all.npy'), embedding_all_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare uid2index.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:47.621699600Z",
     "start_time": "2025-03-10T11:43:47.370246300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156965it [00:00, 664988.87it/s]\n"
     ]
    }
   ],
   "source": [
    "uid2index = {}\n",
    "\n",
    "with open(os.path.join(data_path, 'train', 'behaviors.tsv'), 'r') as f:\n",
    "    for l in tqdm(f):\n",
    "        uid = l.strip('\\n').split('\\t')[1]\n",
    "        if uid not in uid2index:\n",
    "            uid2index[uid] = len(uid2index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:47.625703800Z",
     "start_time": "2025-03-10T11:43:47.618472Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'uid2index.pkl'), 'wb') as f:\n",
    "    pickle.dump(uid2index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:47.629088Z",
     "start_time": "2025-03-10T11:43:47.625703800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'vert_num': 17,\n 'subvert_num': 17,\n 'word_num': 31029,\n 'word_num_all': 55028,\n 'embedding_exist_num': 29081,\n 'embedding_exist_num_all': 48422,\n 'uid2index': 50000}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_state = {\n",
    "    'vert_num': len(vert_dict),\n",
    "    'subvert_num': len(subvert_dict),\n",
    "    'word_num': len(word_dict),\n",
    "    'word_num_all': len(word_dict_all),\n",
    "    'embedding_exist_num': len(exist_word),\n",
    "    'embedding_exist_num_all': len(exist_all_word),\n",
    "    'uid2index': len(uid2index)\n",
    "}\n",
    "utils_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:47.637789600Z",
     "start_time": "2025-03-10T11:43:47.629088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "vert_num",
       "data": 17,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "vert_num",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "subvert_num",
       "data": 17,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "subvert_num",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "word_num",
       "data": 31029,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "word_num",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "word_num_all",
       "data": 55028,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "word_num_all",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "embedding_exist_num",
       "data": 29081,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "embedding_exist_num",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "embedding_exist_num_all",
       "data": 48422,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "embedding_exist_num_all",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "name": "uid2index",
       "data": 50000,
       "encoder": "json"
      }
     },
     "metadata": {
      "notebook_utils": {
       "name": "uid2index",
       "data": true,
       "display": false
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results for tests - ignore this cell\n",
    "store_metadata(\"vert_num\", len(vert_dict))\n",
    "store_metadata(\"subvert_num\", len(subvert_dict))\n",
    "store_metadata(\"word_num\", len(word_dict))\n",
    "store_metadata(\"word_num_all\", len(word_dict_all))\n",
    "store_metadata(\"embedding_exist_num\", len(exist_word))\n",
    "store_metadata(\"embedding_exist_num_all\", len(exist_all_word))\n",
    "store_metadata(\"uid2index\", len(uid2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based filtering      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:52.962064500Z",
     "start_time": "2025-03-10T11:43:47.637789600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_24544\\2062227657.py:24: DeprecationWarning: `awesome_cossim_topn` function will be removed and (partially) replaced with `sp_matmul_topn`. See the migration guide at 'https://github.com/ing-bank/sparse_dot_topn#readme'.\n",
      "  cosine_sim = awesome_cossim_topn(tfidf_matrix_sparse, tfidf_matrix_sparse.T, top_n, threshold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Articles:\n",
      "\n",
      "1. N9056: this is what queen elizabeth is doing about the prince william prince harry feud (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "2. N60671: prince charles teared up when prince william talked about succeeding him (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "3. N38133: the cutest photos of royal children and their beloved nannies from prince george to the queen (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "4. N43522: prince charles is getting into fashion (Genre: lifestyle, Subgenre: lifestylevideo)\n",
      "5. N63174: prince albert on twins jacques and gabriella they re starting to say , are we there yet ? (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "6. N51725: prince charles looks in awe of master archie at christening (Genre: video, Subgenre: lifestyle)\n",
      "7. N18530: all the photos of prince charles s trip to japan for emperor naruhito s enthronement ceremony (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "8. N43301: see all the best photos of prince charles s trip to india (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "9. N57591: prince charles is getting into the fashion business (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "10. N19639: 50 worst habits for belly fat (Genre: health, Subgenre: weightloss)\n",
      "11. N61837: the cost of trump s aid freeze in the trenches of ukraine s war (Genre: news, Subgenre: newsworld)\n",
      "12. N53526: i was an nba wife . here s how it affected my mental health . (Genre: health, Subgenre: voices)\n",
      "13. N38324: how to get rid of skin tags , according to a dermatologist (Genre: health, Subgenre: medical)\n",
      "14. N2073: should nfl be able to fine players for criticizing officiating ? (Genre: sports, Subgenre: football_nfl)\n",
      "15. N49186: it s been orlando s hottest october ever so far , but cooler temperatures on the way (Genre: weather, Subgenre: weathertopstories)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(len(news))\n",
    "\n",
    "#  Combine text features for TF-IDF\n",
    "news['combined_text'] = news['vertical'] + ' ' + news['subvertical'] + ' ' + \\\n",
    "                        news['title'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) + ' ' + \\\n",
    "                        news['abstract'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "#  Initialize TF-IDF with stopword removal and feature limit\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news['combined_text'])\n",
    "\n",
    "#  Convert to sparse matrix for efficient computation\n",
    "tfidf_matrix_sparse = csr_matrix(tfidf_matrix)\n",
    "\n",
    "#  Compute sparse cosine similarity using `sparse_dot_topn`\n",
    "top_n = 10  # Keep only top-10 most similar items per article\n",
    "threshold = 0.01  # Ignore weak similarities\n",
    "cosine_sim = awesome_cossim_topn(tfidf_matrix_sparse, tfidf_matrix_sparse.T, top_n, threshold)\n",
    "\n",
    "#  Function to get top-N recommended articles\n",
    "def get_recommendations(article_index, top_n=5):\n",
    "    \"\"\"Returns top-N most similar news articles based on content similarity.\"\"\"\n",
    "    \n",
    "    # Extract similarity scores for the given article\n",
    "    sim_scores = cosine_sim[article_index].toarray().flatten()  # Convert sparse row to dense array\n",
    "\n",
    "    # Sort and retrieve top-N similar articles (excluding itself)\n",
    "    sim_scores = list(enumerate(sim_scores))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]  \n",
    "\n",
    "    # Extract recommended news details\n",
    "    recommended_articles = []\n",
    "    for i in sim_scores:\n",
    "        news_id = news.iloc[i[0]]['newid']  \n",
    "        title = news.iloc[i[0]]['title']\n",
    "        genre = news.iloc[i[0]]['vertical']  \n",
    "        subgenre = news.iloc[i[0]]['subvertical']\n",
    "\n",
    "        # If title is a list, convert to a readable string\n",
    "        if isinstance(title, list):\n",
    "            title = ' '.join(title)\n",
    "\n",
    "        recommended_articles.append(f\"{news_id}: {title} (Genre: {genre}, Subgenre: {subgenre})\")\n",
    "\n",
    "    return recommended_articles\n",
    "\n",
    "#  Function to get top-N recommended news IDs (for evaluation)\n",
    "def get_recommendations_news_ids(article_index, top_n=10):\n",
    "    \"\"\"Returns top-N most similar news article IDs.\"\"\"\n",
    "    \n",
    "    # Extract similarity scores\n",
    "    sim_scores = cosine_sim[article_index].toarray().flatten()  \n",
    "\n",
    "    # Sort and retrieve top-N articles (excluding itself)\n",
    "    sim_scores = list(enumerate(sim_scores))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]  \n",
    "\n",
    "    # Return only the news IDs\n",
    "    recommended_news_ids = [news.iloc[i[0]]['newid'] for i in sim_scores]\n",
    "    return recommended_news_ids\n",
    "\n",
    "#  Example: Get recommendations for the first article\n",
    "recommended_articles = get_recommendations(0, top_n=15)\n",
    "\n",
    "#  Pretty print the results\n",
    "print(\"Recommended Articles:\\n\")\n",
    "for idx, article in enumerate(recommended_articles, start=1):\n",
    "    print(f\"{idx}. {article}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based filtering result check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:53.185291700Z",
     "start_time": "2025-03-10T11:43:52.964462200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing on Validation Data:\n",
      "ID: N2073\n",
      "Title: Should NFL be able to fine players for criticizing officiating?\n",
      "Genre: sports\n",
      "Subgenre: football_nfl\n",
      "\n",
      "Recommended Articles:\n",
      "\n",
      "1. N37948: prescott bad on the nfl if it does not protect mic d up players (Genre: sports, Subgenre: football_nfl)\n",
      "2. N21089: dak prescott bad on the brand if nfl does not protect mic d up players (Genre: sports, Subgenre: football_nfl)\n",
      "3. N3314: 5 nfl breakout players of 2019 (Genre: sports, Subgenre: football_nfl)\n",
      "4. N46662: nfl cheerleaders (Genre: sports, Subgenre: football_nfl)\n",
      "5. N846: nfl week 7 awards is this the best photo ever taken of a nfl player ? (Genre: sports, Subgenre: football_nfl)\n",
      "6. N12200: teams with most and fewest in state players (Genre: sports, Subgenre: football_ncaa)\n",
      "7. N33164: 100 famous nfl players who played for teams you forgot about (Genre: sports, Subgenre: football_nfl)\n",
      "8. N8921: nfl week 6 awards this baffling call in cleveland was the worst of the week (Genre: sports, Subgenre: football_nfl)\n",
      "9. N48792: 7 players to keep eye on as nfl trade deadline nears (Genre: sports, Subgenre: football_nfl)\n",
      "10. N55528: the brands queen elizabeth , prince charles , and prince philip swear by (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "11. N19639: 50 worst habits for belly fat (Genre: health, Subgenre: weightloss)\n",
      "12. N61837: the cost of trump s aid freeze in the trenches of ukraine s war (Genre: news, Subgenre: newsworld)\n",
      "13. N53526: i was an nba wife . here s how it affected my mental health . (Genre: health, Subgenre: voices)\n",
      "14. N38324: how to get rid of skin tags , according to a dermatologist (Genre: health, Subgenre: medical)\n",
      "15. N49186: it s been orlando s hottest october ever so far , but cooler temperatures on the way (Genre: weather, Subgenre: weathertopstories)\n"
     ]
    }
   ],
   "source": [
    "valid_news = pd.read_table(\n",
    "    os.path.join(data_path, 'valid', 'news.tsv'),\n",
    "    names=['newid', 'vertical', 'subvertical', 'title', 'abstract', 'url', 'entities in title', 'entities in abstract'],\n",
    "    usecols=['newid', 'vertical', 'subvertical', 'title', 'abstract']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "article_index = 5  # Choose a random validation article\n",
    "\n",
    "print(\"\\nTesting on Validation Data:\")\n",
    "print(f\"ID: {valid_news.iloc[article_index]['newid']}\")\n",
    "print(f\"Title: {valid_news.iloc[article_index]['title']}\")\n",
    "print(f\"Genre: {valid_news.iloc[article_index]['vertical']}\")\n",
    "print(f\"Subgenre: {valid_news.iloc[article_index]['subvertical']}\\n\")\n",
    "\n",
    "# Get recommendations based on the validation article\n",
    "recommended_articles = get_recommendations(article_index, top_n=15)\n",
    "\n",
    "print(\"Recommended Articles:\\n\")\n",
    "for idx, article in enumerate(recommended_articles, start=1):\n",
    "    print(f\"{idx}. {article}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:53.527894300Z",
     "start_time": "2025-03-10T11:43:53.187954200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User U80234 previously read:\n",
      "N55189 N46039 N51741 N53234 N11276 N264 N40716 N28088 N43955 N6616 N47686 N63573 N38895 N30924 N35671\n",
      "\n",
      "Recommended articles:\n",
      "1. N9056: this is what queen elizabeth is doing about the prince william prince harry feud (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "2. N60671: prince charles teared up when prince william talked about succeeding him (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "3. N38133: the cutest photos of royal children and their beloved nannies from prince george to the queen (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
      "4. N43522: prince charles is getting into fashion (Genre: lifestyle, Subgenre: lifestylevideo)\n",
      "5. N63174: prince albert on twins jacques and gabriella they re starting to say , are we there yet ? (Genre: lifestyle, Subgenre: lifestyleroyals)\n"
     ]
    }
   ],
   "source": [
    "# Load validation impressions \n",
    "valid_behaviors = pd.read_table(\n",
    "    os.path.join(data_path, 'valid', 'behaviors.tsv'),\n",
    "    names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    ")\n",
    "\n",
    "# Extract a sample user's history\n",
    "sample_user = valid_behaviors.iloc[0]\n",
    "\n",
    "print(f\"User {sample_user['user_id']} previously read:\")\n",
    "print(sample_user['history'])\n",
    "\n",
    "print(\"\\nRecommended articles:\")\n",
    "recommended_articles = get_recommendations(0, top_n=5)\n",
    "for idx, article in enumerate(recommended_articles, start=1):\n",
    "    print(f\"{idx}. {article}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_24544\\2238564210.py:9: DeprecationWarning: `awesome_cossim_topn` function will be removed and (partially) replaced with `sp_matmul_topn`. See the migration guide at 'https://github.com/ing-bank/sparse_dot_topn#readme'.\n",
      "  similarity_matrix = awesome_cossim_topn(tfidf_matrix, tfidf_matrix.T, top_n, 0.01)  # Keep top-N scores\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_data(split=\"train\"):\n",
    "    \"\"\"\n",
    "    Loads news and behaviors data for a given dataset split ('train' or 'valid').\n",
    "    Ensures we only evaluate on articles that exist in the dataset.\n",
    "    \"\"\"\n",
    "    news_df = pd.read_table(\n",
    "        os.path.join(data_path, split, 'news.tsv'),\n",
    "        names=['newid', 'vertical', 'subvertical', 'title', 'abstract']\n",
    "    )\n",
    "    \n",
    "    behaviors_df = pd.read_table(\n",
    "        os.path.join(data_path, split, 'behaviors.tsv'),\n",
    "        names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "    )\n",
    "\n",
    "    return news_df, behaviors_df\n",
    "\n",
    "# ✅ Load training dataset\n",
    "train_news, train_behaviors = load_data(split=\"train\")\n",
    "\n",
    "# ✅ Load validation dataset\n",
    "valid_news, valid_behaviors = load_data(split=\"valid\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T14:44:50.291257100Z",
     "start_time": "2025-03-10T14:44:12.679934500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_24544\\2685579187.py:59: DeprecationWarning: `awesome_cossim_topn` function will be removed and (partially) replaced with `sp_matmul_topn`. See the migration guide at 'https://github.com/ing-bank/sparse_dot_topn#readme'.\n",
      "  similarity_matrix = awesome_cossim_topn(tfidf_matrix, tfidf_matrix.T, top_n, 0.01)  # Keep top-N scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 **User Statistics:**\n",
      "🔹 Evaluated Users: 3135\n",
      "🔹 Skipped Users (less than 5 interactions): 49664\n",
      "\n",
      "🔹 **Training Set Evaluation:**\n",
      "📌 Precision@5: 0.0000\n",
      "📌 Recall@5: 0.0000\n",
      "📌 NDCG@5: 0.0000\n",
      "\n",
      "📊 **User Statistics:**\n",
      "🔹 Evaluated Users: 1905\n",
      "🔹 Skipped Users (less than 5 interactions): 49073\n",
      "\n",
      "🔹 **Validation Set Evaluation:**\n",
      "📌 Precision@5: 0.0000\n",
      "📌 Recall@5: 0.0000\n",
      "📌 NDCG@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ✅ Define cache paths\n",
    "PROJECT_DIR = os.getcwd()\n",
    "CACHE_DIR = os.path.join(PROJECT_DIR, \"cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "SIMILARITY_CACHE = os.path.join(CACHE_DIR, \"similarity_cache.pkl\")\n",
    "CLUSTER_CACHE = os.path.join(CACHE_DIR, \"clusters.pkl\")\n",
    "\n",
    "# ✅ Load news and behavior data\n",
    "def load_data(split=\"train\"):\n",
    "    \"\"\"\n",
    "    Loads news and behaviors data for a given dataset split ('train' or 'valid').\n",
    "    Ensures we only evaluate on articles that exist in the dataset.\n",
    "    \"\"\"\n",
    "    news_df = pd.read_table(\n",
    "        os.path.join(data_path, split, 'news.tsv'),\n",
    "        names=['newid', 'vertical', 'subvertical', 'title', 'abstract']\n",
    "    )\n",
    "    \n",
    "    behaviors_df = pd.read_table(\n",
    "        os.path.join(data_path, split, 'behaviors.tsv'),\n",
    "        names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "    )\n",
    "\n",
    "    return news_df, behaviors_df\n",
    "\n",
    "# ✅ Load train & validation datasets separately\n",
    "train_news, train_behaviors = load_data(split=\"train\")\n",
    "valid_news, valid_behaviors = load_data(split=\"valid\")\n",
    "\n",
    "# ✅ Prepare text features\n",
    "def prepare_text_features(news_df):\n",
    "    \"\"\"Prepares TF-IDF text representation for news articles.\"\"\"\n",
    "    news_df['combined_text'] = news_df[['vertical', 'subvertical', 'title', 'abstract']].fillna('').agg(' '.join, axis=1)\n",
    "    return news_df\n",
    "\n",
    "train_news = prepare_text_features(train_news)\n",
    "valid_news = prepare_text_features(valid_news)\n",
    "\n",
    "# ✅ Compute TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_news['combined_text'])\n",
    "tfidf_matrix_valid = tfidf_vectorizer.transform(valid_news['combined_text'])\n",
    "\n",
    "# ✅ Compute sparse cosine similarity using `sparse_dot_topn`\n",
    "def compute_sparse_similarity(tfidf_matrix, top_n=100):\n",
    "    \"\"\"\n",
    "    Computes sparse cosine similarity, keeping only the top `top_n` similarities per row.\n",
    "    \"\"\"\n",
    "    tfidf_matrix = csr_matrix(tfidf_matrix)  # Convert to sparse matrix\n",
    "    similarity_matrix = awesome_cossim_topn(tfidf_matrix, tfidf_matrix.T, top_n, 0.01)  # Keep top-N scores\n",
    "    return similarity_matrix\n",
    "\n",
    "# ✅ Compute sparse similarity matrices\n",
    "train_similarity_matrix = compute_sparse_similarity(tfidf_matrix_train, top_n=100)\n",
    "valid_similarity_matrix = compute_sparse_similarity(tfidf_matrix_valid, top_n=100)\n",
    "\n",
    "# ✅ Clustering for efficiency\n",
    "NUM_CLUSTERS = 5\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "train_news['cluster'] = kmeans.fit_predict(tfidf_matrix_train)\n",
    "clusters = train_news[['newid', 'cluster']].set_index('newid').to_dict()['cluster']\n",
    "\n",
    "# ✅ Define Content-Based Recommender\n",
    "class ContentBasedRecommender:\n",
    "    def __init__(self, news_df, behaviors_df, similarity_matrix):\n",
    "        self.news_df = news_df\n",
    "        self.behaviors_df = behaviors_df\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "\n",
    "    def recommend(self, user_id, N=5):\n",
    "        \"\"\"Recommend top-N articles for a user within the same dataset.\"\"\"\n",
    "        \n",
    "        user_row = self.behaviors_df[self.behaviors_df[\"user_id\"] == user_id]\n",
    "        if user_row.empty or pd.isna(user_row.iloc[0][\"history\"]):\n",
    "            return []  # No history\n",
    "\n",
    "        clicked_articles = user_row.iloc[0][\"history\"].split()\n",
    "        valid_clicked = [a for a in clicked_articles if a in self.news_df[\"newid\"].values]\n",
    "\n",
    "        if not valid_clicked:\n",
    "            return []  # No matching history in dataset\n",
    "\n",
    "        recommended_news_ids = set()\n",
    "        for article in valid_clicked:\n",
    "            article_index = self.news_df[self.news_df[\"newid\"] == article].index[0]\n",
    "            sim_scores = list(zip(self.similarity_matrix[article_index].indices, self.similarity_matrix[article_index].data))\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "            recommended_news_ids.update([self.news_df.iloc[i[0]]['newid'] for i in sim_scores[:N]])\n",
    "\n",
    "        return list(recommended_news_ids)[:N]\n",
    "\n",
    "# ✅ Custom Evaluation Function\n",
    "def evaluate_model(recommender, behaviors_df, K=5, min_interactions=5):\n",
    "    \"\"\"\n",
    "    Evaluates a recommender model while filtering users with fewer than `min_interactions`.\n",
    "    Ensures correct user counts and prints debug information.\n",
    "    \"\"\"\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    skipped_users = set()\n",
    "    evaluated_users = set()\n",
    "\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        if pd.isna(row[\"impressions\"]):\n",
    "            continue\n",
    "\n",
    "        # Extract clicked articles\n",
    "        actual_clicked = {item.split(\"-\")[0] for item in row[\"impressions\"].split() if item.endswith(\"-1\")}\n",
    "\n",
    "        # Skip users with fewer than `min_interactions`\n",
    "        if len(actual_clicked) < min_interactions:\n",
    "            skipped_users.add(user_id)\n",
    "            continue  \n",
    "\n",
    "        evaluated_users.add(user_id)\n",
    "\n",
    "        # Get recommendations\n",
    "        recommended = recommender.recommend(user_id, N=K) or []\n",
    "        recommended = recommended[:K]\n",
    "\n",
    "        # Compute scores\n",
    "        precision_scores.append(len(set(recommended) & actual_clicked) / K if recommended else 0)\n",
    "        recall_scores.append(len(set(recommended) & actual_clicked) / len(actual_clicked) if recommended else 0)\n",
    "        dcg = sum(1 / np.log2(i + 2) for i, item in enumerate(recommended[:K]) if item in actual_clicked)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(actual_clicked), K))) or 1\n",
    "        ndcg_scores.append(dcg / idcg)\n",
    "\n",
    "    avg_precision = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    avg_recall = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "    print(f\"\\n📊 **User Statistics:**\")\n",
    "    print(f\"🔹 Evaluated Users: {len(evaluated_users)}\")\n",
    "    print(f\"🔹 Skipped Users (less than {min_interactions} interactions): {len(skipped_users)}\")\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "# ✅ Train & Evaluate on Training Dataset\n",
    "train_recommender = ContentBasedRecommender(train_news, train_behaviors, train_similarity_matrix)\n",
    "precision_train, recall_train, ndcg_train = evaluate_model(train_recommender, train_behaviors, 5, min_interactions=5)\n",
    "\n",
    "print(\"\\n🔹 **Training Set Evaluation:**\")\n",
    "print(f\"📌 Precision@5: {precision_train:.4f}\")\n",
    "print(f\"📌 Recall@5: {recall_train:.4f}\")\n",
    "print(f\"📌 NDCG@5: {ndcg_train:.4f}\")\n",
    "\n",
    "# ✅ Train & Evaluate on Validation Dataset\n",
    "valid_recommender = ContentBasedRecommender(valid_news, valid_behaviors, valid_similarity_matrix)\n",
    "precision_valid, recall_valid, ndcg_valid = evaluate_model(valid_recommender, valid_behaviors, 5, min_interactions=5)\n",
    "\n",
    "print(\"\\n🔹 **Validation Set Evaluation:**\")\n",
    "print(f\"📌 Precision@5: {precision_valid:.4f}\")\n",
    "print(f\"📌 Recall@5: {recall_valid:.4f}\")\n",
    "print(f\"📌 NDCG@5: {ndcg_valid:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T15:00:29.071382800Z",
     "start_time": "2025-03-10T14:56:59.933094Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Selected Article:**\n",
      "📌 ID: Dispose of unwanted prescription drugs during the DEA's Take Back Day\n",
      "📰 Title: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.992, \"OccurrenceOffsets\": [50], \"SurfaceForms\": [\"DEA\"]}]\n",
      "📄 Abstract: []\n",
      "\n",
      "🔍 **Top 5 Recommended Articles:**\n",
      "\n",
      "⭐ **Recommendation 1 (Similarity: 1.0000)**\n",
      "📌 ID: Dispose of unwanted prescription drugs during the DEA's Take Back Day\n",
      "📰 Title: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.992, \"OccurrenceOffsets\": [50], \"SurfaceForms\": [\"DEA\"]}]\n",
      "📄 Abstract: []\n",
      "\n",
      "⭐ **Recommendation 2 (Similarity: 0.8527)**\n",
      "📌 ID: The Drug Enforcement Administration warns of lethal fentanyl-laced pills\n",
      "📰 Title: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Drug Enforcement Administration\"]}]\n",
      "📄 Abstract: []\n",
      "\n",
      "⭐ **Recommendation 3 (Similarity: 0.8364)**\n",
      "📌 ID: Downtown jewelry store raided by DEA in cocaine and fentanyl trafficking investigation\n",
      "📰 Title: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.992, \"OccurrenceOffsets\": [33], \"SurfaceForms\": [\"DEA\"]}]\n",
      "📄 Abstract: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.992, \"OccurrenceOffsets\": [58], \"SurfaceForms\": [\"DEA\"]}]\n",
      "\n",
      "⭐ **Recommendation 4 (Similarity: 0.7841)**\n",
      "📌 ID: DEA to accept vaping devices at drug take-back day, as lung injuries top 1,600\n",
      "📰 Title: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"DEA\"]}]\n",
      "📄 Abstract: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Drug Enforcement Administration\"]}]\n",
      "\n",
      "⭐ **Recommendation 5 (Similarity: 0.7031)**\n",
      "📌 ID: Chronic Pain Sufferers Beg the DEA to Reconsider Prescription Opioid Cuts\n",
      "📰 Title: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"DEA\"]}]\n",
      "📄 Abstract: [{\"Label\": \"Drug Enforcement Administration\", \"Type\": \"O\", \"WikidataId\": \"Q622899\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"DEA\"]}]\n"
     ]
    }
   ],
   "source": [
    "sample_article_index = 1\n",
    "sample_article_id = valid_news.iloc[sample_article_index][\"newid\"]\n",
    "sample_article_title = valid_news.iloc[sample_article_index][\"title\"]\n",
    "sample_article_abstract = valid_news.iloc[sample_article_index][\"abstract\"]\n",
    "\n",
    "# Get top 5 recommendations for this article\n",
    "sim_scores = list(zip(valid_similarity_matrix[sample_article_index].indices, valid_similarity_matrix[sample_article_index].data))\n",
    "sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Print the selected article\n",
    "print(f\"\\n🔍 **Selected Article:**\")\n",
    "print(f\"📌 ID: {sample_article_id}\")\n",
    "print(f\"📰 Title: {sample_article_title}\")\n",
    "print(f\"📄 Abstract: {sample_article_abstract}\")\n",
    "print(f\"\\n🔍 **Top 5 Recommended Articles:**\")\n",
    "\n",
    "# Print recommended articles\n",
    "for idx, (rec_index, score) in enumerate(sim_scores):\n",
    "    rec_article = valid_news.iloc[rec_index]\n",
    "    print(f\"\\n⭐ **Recommendation {idx+1} (Similarity: {score:.4f})**\")\n",
    "    print(f\"📌 ID: {rec_article['newid']}\")\n",
    "    print(f\"📰 Title: {rec_article['title']}\")\n",
    "    print(f\"📄 Abstract: {rec_article['abstract']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T15:15:33.069351900Z",
     "start_time": "2025-03-10T15:15:33.059942600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 **Total Clicked Articles in Validation:** 2212\n",
      "✅ **Clicked Articles Found in Training Data:** 0\n",
      "❌ **Clicked Articles Missing from Training:** 2212\n"
     ]
    }
   ],
   "source": [
    "# ✅ Extract all unique clicked articles from validation\n",
    "clicked_articles_in_valid = set()\n",
    "\n",
    "for _, row in valid_behaviors_df.iterrows():\n",
    "    if pd.isna(row[\"impressions\"]):\n",
    "        continue\n",
    "    clicked_articles = {item.split(\"-\")[0] for item in row[\"impressions\"].split() if item.endswith(\"-1\")}\n",
    "    clicked_articles_in_valid.update(clicked_articles)\n",
    "\n",
    "# ✅ Extract all unique articles in training\n",
    "train_articles = set(train_news[\"newid\"].unique())\n",
    "\n",
    "# ✅ Find missing articles\n",
    "missing_articles = clicked_articles_in_valid - train_articles\n",
    "\n",
    "print(f\"🔹 **Total Clicked Articles in Validation:** {len(clicked_articles_in_valid)}\")\n",
    "print(f\"✅ **Clicked Articles Found in Training Data:** {len(clicked_articles_in_valid & train_articles)}\")\n",
    "print(f\"❌ **Clicked Articles Missing from Training:** {len(missing_articles)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T13:57:10.689325400Z",
     "start_time": "2025-03-10T13:57:09.048276900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User U86141\n",
      "    Clicked: {'N44621'}\n",
      "    Recommended: []\n",
      "   🎯 Matches: set()\n",
      "❌ No matches found for this user!\n",
      "❌ No matching clicked articles found for User U87658."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 102\u001B[0m\n\u001B[0;32m     99\u001B[0m recommender \u001B[38;5;241m=\u001B[39m ContentBasedRecommender(train_news, valid_news, similarity_matrix, clusters, valid_behaviors_df)\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# ✅ Evaluate\u001B[39;00m\n\u001B[1;32m--> 102\u001B[0m precision, recall, ndcg \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecommender\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_behaviors_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;66;03m# ✅ Print Results\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m🔹 **Final Evaluation Results:**\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive - NTNU\\NTNU\\4 år\\Recommender systems\\Project\\utils\\evaluation.py:69\u001B[0m, in \u001B[0;36mevaluate_model\u001B[1;34m(recommender, behaviors_df, K)\u001B[0m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Get recommendations\u001B[39;00m\n\u001B[1;32m---> 69\u001B[0m recommended \u001B[38;5;241m=\u001B[39m \u001B[43mrecommender\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecommend\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mK\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m []\n\u001B[0;32m     70\u001B[0m recommended \u001B[38;5;241m=\u001B[39m recommended[:K]\n\u001B[0;32m     72\u001B[0m matches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(recommended) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(actual_clicked)\n",
      "Cell \u001B[1;32mIn[28], line 81\u001B[0m, in \u001B[0;36mContentBasedRecommender.recommend\u001B[1;34m(self, user_id, N)\u001B[0m\n\u001B[0;32m     78\u001B[0m valid_clicked \u001B[38;5;241m=\u001B[39m [a \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m clicked_articles \u001B[38;5;28;01mif\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_news[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnewid\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues]\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_clicked:\n\u001B[1;32m---> 81\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m❌ No matching clicked articles found for User \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43muser_id\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[0;32m     84\u001B[0m recommended_news_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\iostream.py:694\u001B[0m, in \u001B[0;36mOutStream.write\u001B[1;34m(self, string)\u001B[0m\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_thread\u001B[38;5;241m.\u001B[39mschedule(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flush)\n\u001B[0;32m    693\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 694\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_schedule_flush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(string)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\iostream.py:590\u001B[0m, in \u001B[0;36mOutStream._schedule_flush\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_schedule_in_thread\u001B[39m():\n\u001B[0;32m    588\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_io_loop\u001B[38;5;241m.\u001B[39mcall_later(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflush_interval, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flush)\n\u001B[1;32m--> 590\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpub_thread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_schedule_in_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\iostream.py:267\u001B[0m, in \u001B[0;36mIOPubThread.schedule\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_events\u001B[38;5;241m.\u001B[39mappend(f)\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;66;03m# wake event thread (message content is ignored)\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event_pipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    269\u001B[0m     f()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\zmq\\sugar\\socket.py:701\u001B[0m, in \u001B[0;36mSocket.send\u001B[1;34m(self, data, flags, copy, track, routing_id, group)\u001B[0m\n\u001B[0;32m    694\u001B[0m         data \u001B[38;5;241m=\u001B[39m zmq\u001B[38;5;241m.\u001B[39mFrame(\n\u001B[0;32m    695\u001B[0m             data,\n\u001B[0;32m    696\u001B[0m             track\u001B[38;5;241m=\u001B[39mtrack,\n\u001B[0;32m    697\u001B[0m             copy\u001B[38;5;241m=\u001B[39mcopy \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    698\u001B[0m             copy_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy_threshold,\n\u001B[0;32m    699\u001B[0m         )\n\u001B[0;32m    700\u001B[0m     data\u001B[38;5;241m.\u001B[39mgroup \u001B[38;5;241m=\u001B[39m group\n\u001B[1;32m--> 701\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrack\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrack\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m_zmq.py:1092\u001B[0m, in \u001B[0;36mzmq.backend.cython._zmq.Socket.send\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_zmq.py:1140\u001B[0m, in \u001B[0;36mzmq.backend.cython._zmq.Socket.send\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_zmq.py:1339\u001B[0m, in \u001B[0;36mzmq.backend.cython._zmq._send_copy\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_zmq.py:160\u001B[0m, in \u001B[0;36mzmq.backend.cython._zmq._check_rc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.evaluation import evaluate_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ✅ Define cache file paths\n",
    "PROJECT_DIR = os.getcwd()\n",
    "CACHE_DIR = os.path.join(PROJECT_DIR, \"cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "SIMILARITY_CACHE = os.path.join(CACHE_DIR, \"similarity_cache.pkl\")\n",
    "CLUSTER_CACHE = os.path.join(CACHE_DIR, \"clusters.pkl\")\n",
    "\n",
    "# ✅ Load train & validation news\n",
    "train_news = pd.read_table(\n",
    "    os.path.join(data_path, 'train', 'news.tsv'),\n",
    "    names=['newid', 'vertical', 'subvertical', 'title', 'abstract']\n",
    ")\n",
    "\n",
    "valid_news = pd.read_table(\n",
    "    os.path.join(data_path, 'valid', 'news.tsv'),\n",
    "    names=['newid', 'vertical', 'subvertical', 'title', 'abstract']\n",
    ")\n",
    "\n",
    "# ✅ Prepare text features\n",
    "# ✅ Ensure there are no NaN values in combined_text\n",
    "train_news['combined_text'] = train_news[['vertical', 'subvertical', 'title', 'abstract']].fillna('').agg(' '.join, axis=1)\n",
    "valid_news['combined_text'] = valid_news[['vertical', 'subvertical', 'title', 'abstract']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# ✅ Compute TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_news['combined_text'])\n",
    "tfidf_matrix_valid = tfidf_vectorizer.transform(valid_news['combined_text'])  # ✅ Only transform for valid\n",
    "\n",
    "# ✅ Compute similarity between validation & training articles\n",
    "if os.path.exists(SIMILARITY_CACHE):\n",
    "    with open(SIMILARITY_CACHE, \"rb\") as f:\n",
    "        similarity_matrix = pickle.load(f)\n",
    "else:\n",
    "    print(\"Computing similarity between validation and training articles...\")\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix_valid, tfidf_matrix_train)\n",
    "    with open(SIMILARITY_CACHE, \"wb\") as f:\n",
    "        pickle.dump(similarity_matrix, f)\n",
    "\n",
    "# ✅ Clustering for efficient search\n",
    "NUM_CLUSTERS = 5  # ✅ Reduce clusters for better coverage\n",
    "if os.path.exists(CLUSTER_CACHE):\n",
    "    with open(CLUSTER_CACHE, \"rb\") as f:\n",
    "        clusters = pickle.load(f)\n",
    "else:\n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "    train_news['cluster'] = kmeans.fit_predict(tfidf_matrix_train)\n",
    "    clusters = train_news[['newid', 'cluster']].set_index('newid').to_dict()['cluster']\n",
    "    with open(CLUSTER_CACHE, \"wb\") as f:\n",
    "        pickle.dump(clusters, f)\n",
    "\n",
    "# ✅ Define Content-Based Recommender\n",
    "class ContentBasedRecommender:\n",
    "    def __init__(self, train_news, valid_news, similarity_matrix, clusters, behaviors_df):\n",
    "        self.train_news = train_news\n",
    "        self.valid_news = valid_news\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        self.clusters = clusters\n",
    "        self.behaviors_df = behaviors_df\n",
    "\n",
    "    def recommend(self, user_id, N=10):\n",
    "        \"\"\"Recommend top-N articles for a user.\"\"\"\n",
    "        \n",
    "        user_row = self.behaviors_df[self.behaviors_df[\"user_id\"] == user_id]\n",
    "        if user_row.empty or pd.isna(user_row.iloc[0][\"history\"]):\n",
    "            return []  # No history\n",
    "\n",
    "        clicked_articles = user_row.iloc[0][\"history\"].split()\n",
    "        valid_clicked = [a for a in clicked_articles if a in self.train_news[\"newid\"].values]\n",
    "\n",
    "        if not valid_clicked:\n",
    "            print(f\"❌ No matching clicked articles found for User {user_id}.\")\n",
    "            return []\n",
    "\n",
    "        recommended_news_ids = set()\n",
    "        for article in valid_clicked:\n",
    "            article_index = self.train_news[self.train_news[\"newid\"] == article].index[0]\n",
    "            sim_scores = sorted(enumerate(self.similarity_matrix[:, article_index]), key=lambda x: x[1], reverse=True)\n",
    "            recommended_news_ids.update([self.valid_news.iloc[i[0]]['newid'] for i in sim_scores[:N]])\n",
    "\n",
    "        return list(recommended_news_ids)[:N]\n",
    "\n",
    "# ✅ Load validation behaviors\n",
    "valid_behaviors_df = pd.read_table(\n",
    "    os.path.join(data_path, 'valid', 'behaviors.tsv'),\n",
    "    names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    ")\n",
    "\n",
    "# ✅ Initialize Recommender\n",
    "recommender = ContentBasedRecommender(train_news, valid_news, similarity_matrix, clusters, valid_behaviors_df)\n",
    "\n",
    "# ✅ Evaluate\n",
    "precision, recall, ndcg = evaluate_model(recommender, valid_behaviors_df, 10)\n",
    "\n",
    "# ✅ Print Results\n",
    "print(f\"\\n🔹 **Final Evaluation Results:**\")\n",
    "print(f\"📌 Precision@10: {precision:.4f}\")\n",
    "print(f\"📌 Recall@10: {recall:.4f}\")\n",
    "print(f\"📌 NDCG@10: {ndcg:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:53.677864900Z",
     "start_time": "2025-03-10T11:43:53.673969600Z"
    }
   },
   "outputs": [],
   "source": [
    "class DummyRecommender:\n",
    "    def recommend(self, user_id, N=5):\n",
    "        \"\"\"Always returns the actual clicked articles (perfect recommender).\"\"\"\n",
    "        user_row = valid_behaviors_df[valid_behaviors_df[\"user_id\"] == user_id]\n",
    "        if user_row.empty or pd.isna(user_row.iloc[0][\"impressions\"]):\n",
    "            return []\n",
    "        actual_clicked = {item.split(\"-\")[0] for item in user_row.iloc[0][\"impressions\"].split() if item.endswith(\"-1\")}\n",
    "        return list(actual_clicked)[:N]\n",
    "\n",
    "# ✅ Evaluate Dummy Model\n",
    "dummy_recommender = DummyRecommender()\n",
    "precision, recall, ndcg = evaluate_model(dummy_recommender, valid_behaviors_df, 5)\n",
    "\n",
    "print(f\"\\n **Dummy Model (Perfect Recommendations) Results:**\")\n",
    "print(f\" Precision@10: {precision:.4f}\")\n",
    "print(f\" Recall@10: {recall:.4f}\")\n",
    "print(f\" NDCG@10: {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-10T11:43:53.673969600Z"
    }
   },
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T11:43:53.677864900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T11:43:53.679214600Z",
     "start_time": "2025-03-10T11:43:53.677864900Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
