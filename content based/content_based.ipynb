{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<i>Copyright (c) Recommenders contributors.</i>\n",
                "\n",
                "<i>Licensed under the MIT License.</i>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  MIND Utils Generation\n",
                "\n",
                "MIND dataset\\[1\\] is a large-scale English news dataset. It was collected from anonymized behavior logs of Microsoft News website. MIND contains 1,000,000 users, 161,013 news articles and 15,777,377 impression logs. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression.\n",
                "\n",
                "Many news recommendation methods use word embeddings, news vertical embeddings, news subvertical embeddings and user id embedding. Therefore, it is necessary to generate a word dictionary, a vertical dictionary, a subvertical dictionary and a `userid` dictionary to convert words, news verticals, subverticals and user ids from strings to indexes. To use the pretrain word embedding, an embedding matrix is generated as the initial weight of the word embedding layer.\n",
                "\n",
                "This notebook gives examples about how to generate:\n",
                "* `word_dict.pkl`: convert the words in news titles into indexes.\n",
                "* `word_dict_all.pkl`: convert the words in news titles and abstracts into indexes.\n",
                "* `embedding.npy`: pretrained word embedding matrix of words in word_dict.pkl\n",
                "* `embedding_all.npy`: pretrained embedding matrix of words in word_dict_all.pkl\n",
                "* `vert_dict.pkl`: convert news verticals into indexes.\n",
                "* `subvert_dict.pkl`: convert news subverticals into indexes.\n",
                "* `uid2index.pkl`: convert user ids into indexes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import pickle\n",
                "from collections import Counter\n",
                "from tempfile import TemporaryDirectory\n",
                "\n",
                "from recommenders.datasets.mind import (download_mind,\n",
                "                                     extract_mind,\n",
                "                                     download_and_extract_glove,\n",
                "                                     load_glove_matrix,\n",
                "                                     word_tokenize\n",
                "                                    )\n",
                "from recommenders.datasets.download_utils import unzip_file\n",
                "from recommenders.utils.notebook_utils import store_metadata\n",
                "\n",
                "print(\"System version: {}\".format(sys.version))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# MIND sizes: \"demo\", \"small\" or \"large\"\n",
                "mind_type=\"small\" \n",
                "# word_embedding_dim should be in [50, 100, 200, 300]\n",
                "word_embedding_dim = 300"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 51.8k/51.8k [00:20<00:00, 2.47kKB/s]\n",
                        "100%|██████████| 30.2k/30.2k [00:05<00:00, 5.79kKB/s]\n"
                    ]
                }
            ],
            "source": [
                "tmpdir = TemporaryDirectory()\n",
                "data_path = tmpdir.name\n",
                "train_zip, valid_zip = download_mind(size=mind_type, dest_path=data_path)\n",
                "unzip_file(train_zip, os.path.join(data_path, 'train'), clean_zip_file=False)\n",
                "unzip_file(valid_zip, os.path.join(data_path, 'valid'), clean_zip_file=False)\n",
                "output_path = os.path.join(data_path, 'utils')\n",
                "os.makedirs(output_path, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare utils of news\n",
                "\n",
                "* word dictionary\n",
                "* vertical dictionary\n",
                "* subvetical dictionary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'numpy.rec'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m news \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnews.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnewid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvertical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubvertical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentities in title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentities in abstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                     \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnewid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvertical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubvertical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(news))\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1405\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1392\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1393\u001b[0m     dialect,\n\u001b[0;32m   1394\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1402\u001b[0m )\n\u001b[0;32m   1403\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:444\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m Series(data, index\u001b[38;5;241m=\u001b[39mcolumns, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m--> 444\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[43marrays\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     index \u001b[38;5;241m=\u001b[39m _extract_index(arrays[\u001b[38;5;241m~\u001b[39mmissing])\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:5775\u001b[0m, in \u001b[0;36mSeries.isna\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5773\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39misna, klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m   5774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m-> 5775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:8754\u001b[0m, in \u001b[0;36mNDFrame.isna\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   8693\u001b[0m \u001b[38;5;129m@doc\u001b[39m(klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   8694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   8695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8696\u001b[0m \u001b[38;5;124;03m    Detect missing values.\u001b[39;00m\n\u001b[0;32m   8697\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8752\u001b[0m \u001b[38;5;124;03m    dtype: bool\u001b[39;00m\n\u001b[0;32m   8753\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 8754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misna\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:178\u001b[0m, in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:216\u001b[0m, in \u001b[0;36m_isna\u001b[1;34m(obj, inf_as_na)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isna_array(obj\u001b[38;5;241m.\u001b[39m_values, inf_as_na\u001b[38;5;241m=\u001b[39minf_as_na)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCSeries):\n\u001b[1;32m--> 216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_isna_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# box\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     result \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:288\u001b[0m, in \u001b[0;36m_isna_array\u001b[1;34m(values, inf_as_na)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;66;03m# \"Union[ndarray[Any, Any], ExtensionArraySupportsAnyAll]\", variable has\u001b[39;00m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;66;03m# type \"ndarray[Any, dtype[bool_]]\")\u001b[39;00m\n\u001b[0;32m    287\u001b[0m         result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39misna()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241m.\u001b[39mrecarray):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# GH 48526\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     result \u001b[38;5;241m=\u001b[39m _isna_recarray_dtype(values, inf_as_na\u001b[38;5;241m=\u001b[39minf_as_na)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_string_or_object_np_dtype(values\u001b[38;5;241m.\u001b[39mdtype):\n",
                        "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\__init__.py:381\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current Numpy installation (\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m) fails to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass simple sanity checks. This can be caused for example \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby incorrect BLAS library being linked in, or by mixing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage managers (pip, conda, apt, ...). Search closed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy issues for similar problems.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__file__\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.rec'"
                    ]
                }
            ],
            "source": [
                "news = pd.read_table(os.path.join(data_path, 'train', 'news.tsv'),\n",
                "                     names=['newid', 'vertical', 'subvertical', 'title',\n",
                "                            'abstract', 'url', 'entities in title', 'entities in abstract'],\n",
                "                     usecols = ['newid','vertical', 'subvertical', 'title', 'abstract'])\n",
                "\n",
                "print(len(news))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>newid</th>\n",
                            "      <th>vertical</th>\n",
                            "      <th>subvertical</th>\n",
                            "      <th>title</th>\n",
                            "      <th>abstract</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>N55528</td>\n",
                            "      <td>lifestyle</td>\n",
                            "      <td>lifestyleroyals</td>\n",
                            "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
                            "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>N19639</td>\n",
                            "      <td>health</td>\n",
                            "      <td>weightloss</td>\n",
                            "      <td>50 Worst Habits For Belly Fat</td>\n",
                            "      <td>These seemingly harmless habits are holding yo...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>N61837</td>\n",
                            "      <td>news</td>\n",
                            "      <td>newsworld</td>\n",
                            "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
                            "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>N53526</td>\n",
                            "      <td>health</td>\n",
                            "      <td>voices</td>\n",
                            "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
                            "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>N38324</td>\n",
                            "      <td>health</td>\n",
                            "      <td>medical</td>\n",
                            "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
                            "      <td>They seem harmless, but there's a very good re...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "    newid   vertical      subvertical  \\\n",
                            "0  N55528  lifestyle  lifestyleroyals   \n",
                            "1  N19639     health       weightloss   \n",
                            "2  N61837       news        newsworld   \n",
                            "3  N53526     health           voices   \n",
                            "4  N38324     health          medical   \n",
                            "\n",
                            "                                               title  \\\n",
                            "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
                            "1                      50 Worst Habits For Belly Fat   \n",
                            "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
                            "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
                            "4  How to Get Rid of Skin Tags, According to a De...   \n",
                            "\n",
                            "                                            abstract  \n",
                            "0  Shop the notebooks, jackets, and more that the...  \n",
                            "1  These seemingly harmless habits are holding yo...  \n",
                            "2  Lt. Ivan Molchanets peeked over a parapet of s...  \n",
                            "3  I felt like I was a fraud, and being an NBA wi...  \n",
                            "4  They seem harmless, but there's a very good re...  "
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "news.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "news_vertical = news.vertical.drop_duplicates().reset_index(drop=True)\n",
                "vert_dict_inv = news_vertical.to_dict()\n",
                "vert_dict = {v: k+1 for k, v in vert_dict_inv.items()}\n",
                "\n",
                "news_subvertical = news.subvertical.drop_duplicates().reset_index(drop=True)\n",
                "subvert_dict_inv = news_subvertical.to_dict()\n",
                "subvert_dict = {v: k+1 for k, v in vert_dict_inv.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "news.title = news.title.apply(word_tokenize)\n",
                "news.abstract = news.abstract.apply(word_tokenize)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 51282/51282 [00:01<00:00, 25974.08it/s]\n"
                    ]
                }
            ],
            "source": [
                "word_cnt = Counter()\n",
                "word_cnt_all = Counter()\n",
                "\n",
                "for i in tqdm(range(len(news))):\n",
                "    word_cnt.update(news.loc[i]['title'])\n",
                "    word_cnt_all.update(news.loc[i]['title'])\n",
                "    word_cnt_all.update(news.loc[i]['abstract'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word_dict = {k: v+1 for k, v in zip(word_cnt, range(len(word_cnt)))}\n",
                "word_dict_all = {k: v+1 for k, v in zip(word_cnt_all, range(len(word_cnt_all)))}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(os.path.join(output_path, 'vert_dict.pkl'), 'wb') as f:\n",
                "    pickle.dump(vert_dict, f)\n",
                "    \n",
                "with open(os.path.join(output_path, 'subvert_dict.pkl'), 'wb') as f:\n",
                "    pickle.dump(subvert_dict, f)\n",
                "\n",
                "with open(os.path.join(output_path, 'word_dict.pkl'), 'wb') as f:\n",
                "    pickle.dump(word_dict, f)\n",
                "    \n",
                "with open(os.path.join(output_path, 'word_dict_all.pkl'), 'wb') as f:\n",
                "    pickle.dump(word_dict_all, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare embedding matrixs\n",
                "* embedding.npy\n",
                "* embedding_all.npy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 842k/842k [01:14<00:00, 11.3kKB/s] \n"
                    ]
                }
            ],
            "source": [
                "glove_path = download_and_extract_glove(data_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "400001it [00:05, 71162.62it/s]\n",
                        "400001it [00:05, 76498.90it/s] \n"
                    ]
                }
            ],
            "source": [
                "embedding_matrix, exist_word = load_glove_matrix(glove_path, word_dict, word_embedding_dim)\n",
                "embedding_all_matrix, exist_all_word = load_glove_matrix(glove_path, word_dict_all, word_embedding_dim)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.save(os.path.join(output_path, 'embedding.npy'), embedding_matrix)\n",
                "np.save(os.path.join(output_path, 'embedding_all.npy'), embedding_all_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare uid2index.pkl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "156965it [00:00, 658058.85it/s]\n"
                    ]
                }
            ],
            "source": [
                "uid2index = {}\n",
                "\n",
                "with open(os.path.join(data_path, 'train', 'behaviors.tsv'), 'r') as f:\n",
                "    for l in tqdm(f):\n",
                "        uid = l.strip('\\n').split('\\t')[1]\n",
                "        if uid not in uid2index:\n",
                "            uid2index[uid] = len(uid2index) + 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(os.path.join(output_path, 'uid2index.pkl'), 'wb') as f:\n",
                "    pickle.dump(uid2index, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'vert_num': 17,\n",
                            " 'subvert_num': 17,\n",
                            " 'word_num': 31029,\n",
                            " 'word_num_all': 55028,\n",
                            " 'embedding_exist_num': 29081,\n",
                            " 'embedding_exist_num_all': 48422,\n",
                            " 'uid2index': 50000}"
                        ]
                    },
                    "execution_count": 170,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "utils_state = {\n",
                "    'vert_num': len(vert_dict),\n",
                "    'subvert_num': len(subvert_dict),\n",
                "    'word_num': len(word_dict),\n",
                "    'word_num_all': len(word_dict_all),\n",
                "    'embedding_exist_num': len(exist_word),\n",
                "    'embedding_exist_num_all': len(exist_all_word),\n",
                "    'uid2index': len(uid2index)\n",
                "}\n",
                "utils_state"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 17,
                            "encoder": "json",
                            "name": "vert_num"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "vert_num"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 17,
                            "encoder": "json",
                            "name": "subvert_num"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "subvert_num"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 31029,
                            "encoder": "json",
                            "name": "word_num"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "word_num"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 55028,
                            "encoder": "json",
                            "name": "word_num_all"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "word_num_all"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 29081,
                            "encoder": "json",
                            "name": "embedding_exist_num"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "embedding_exist_num"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 48422,
                            "encoder": "json",
                            "name": "embedding_exist_num_all"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "embedding_exist_num_all"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 50000,
                            "encoder": "json",
                            "name": "uid2index"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "uid2index"
                        }
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Record results for tests - ignore this cell\n",
                "store_metadata(\"vert_num\", len(vert_dict))\n",
                "store_metadata(\"subvert_num\", len(subvert_dict))\n",
                "store_metadata(\"word_num\", len(word_dict))\n",
                "store_metadata(\"word_num_all\", len(word_dict_all))\n",
                "store_metadata(\"embedding_exist_num\", len(exist_word))\n",
                "store_metadata(\"embedding_exist_num_all\", len(exist_all_word))\n",
                "store_metadata(\"uid2index\", len(uid2index))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Content based filtering      "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "51282\n",
                        "Recommended Articles:\n",
                        "\n",
                        "1. N9056: this is what queen elizabeth is doing about the prince william prince harry feud (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "2. N60671: prince charles teared up when prince william talked about succeeding him (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "3. N38133: the cutest photos of royal children and their beloved nannies from prince george to the queen (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "4. N18530: all the photos of prince charles s trip to japan for emperor naruhito s enthronement ceremony (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "5. N63823: prince charles hit by one of the most incredible art hoaxes in royal history (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "6. N63174: prince albert on twins jacques and gabriella they re starting to say , are we there yet ? (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "7. N63495: 65 photos of prince charles you ve probably never seen before (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "8. N43522: prince charles is getting into fashion (Genre: lifestyle, Subgenre: lifestylevideo)\n",
                        "9. N57591: prince charles is getting into the fashion business (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "10. N42777: prince george s royal life in photos (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "11. N23273: best looks queen maxima of the netherlands (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "12. N51725: prince charles looks in awe of master archie at christening (Genre: video, Subgenre: lifestyle)\n",
                        "13. N43301: see all the best photos of prince charles s trip to india (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "14. N53921: see all the photos of queen elizabeth and camilla at westminster abbey this morning (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "15. N28614: a look back at prince charles and camilla s relationship through the years (Genre: lifestyle, Subgenre: lifestyleroyals)\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "print(len(news))\n",
                "# Load news dataset\n",
                "news['combined_text'] = news['vertical'] + ' ' + news['subvertical'] + ' ' + \\\n",
                "                        news['title'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) + ' ' + \\\n",
                "                        news['abstract'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
                "\n",
                "\n",
                "# Initialize TF-IDF Vectorizer\n",
                "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
                "\n",
                "# Compute TF-IDF matrix\n",
                "tfidf_matrix = tfidf_vectorizer.fit_transform(news['combined_text'])\n",
                "\n",
                "# Compute cosine similarity matrix\n",
                "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
                "\n",
                "# Function to get top N recommendations\n",
                "def get_recommendations(article_index, top_n=5):\n",
                "    \"\"\"Returns top-N most similar news articles based on content similarity, formatted properly.\"\"\"\n",
                "    sim_scores = list(enumerate(cosine_sim[article_index]))\n",
                "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]  # Exclude self\n",
                "    \n",
                "    recommended_articles = []\n",
                "    for i in sim_scores:\n",
                "        news_id = news.iloc[i[0]]['newid']  # Get news ID\n",
                "        title = news.iloc[i[0]]['title']\n",
                "        genre = news.iloc[i[0]]['vertical']  # Get genre\n",
                "        subgenre = news.iloc[i[0]]['subvertical']  # Get subgenre\n",
                "        \n",
                "        # If title is a list of words, join into a readable string\n",
                "        if isinstance(title, list):\n",
                "            title = ' '.join(title)\n",
                "        \n",
                "        recommended_articles.append(f\"{news_id}: {title} (Genre: {genre}, Subgenre: {subgenre})\")\n",
                "    \n",
                "    return recommended_articles\n",
                "\n",
                "# Example: Get recommendations for first article\n",
                "recommended_articles = get_recommendations(0, top_n=15)\n",
                "\n",
                "# Pretty print the results\n",
                "print(\"Recommended Articles:\\n\")\n",
                "for idx, article in enumerate(recommended_articles, start=1):\n",
                "    print(f\"{idx}. {article}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Content based filtering validation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Testing on Validation Data:\n",
                        "ID: N2073\n",
                        "Title: Should NFL be able to fine players for criticizing officiating?\n",
                        "Genre: sports\n",
                        "Subgenre: football_nfl\n",
                        "\n",
                        "Recommended Articles:\n",
                        "\n",
                        "1. N61576: nfl fines baker mayfield for stating the obvious (Genre: sports, Subgenre: football_nfl)\n",
                        "2. N29891: nfl officiating stinks . here are 10 ways to fix it . (Genre: sports, Subgenre: football_nfl)\n",
                        "3. N46662: nfl cheerleaders (Genre: sports, Subgenre: football_nfl)\n",
                        "4. N3314: 5 nfl breakout players of 2019 (Genre: sports, Subgenre: football_nfl)\n",
                        "5. N51783: retired eagles de chris long calls officiating a mess , says nfl needs to do something (Genre: sports, Subgenre: football_nfl)\n",
                        "6. N36282: nfl sending message with multiple fines for criticizing referees (Genre: sports, Subgenre: football_nfl)\n",
                        "7. N12200: teams with most and fewest in state players (Genre: sports, Subgenre: football_ncaa)\n",
                        "8. N33164: 100 famous nfl players who played for teams you forgot about (Genre: sports, Subgenre: football_nfl)\n",
                        "9. N43525: nfl cracks down on criticizing refs with fines for baker mayfield , clay matthews (Genre: sports, Subgenre: football_nfl)\n",
                        "10. N37948: prescott bad on the nfl if it does not protect mic d up players (Genre: sports, Subgenre: football_nfl)\n",
                        "11. N48792: 7 players to keep eye on as nfl trade deadline nears (Genre: sports, Subgenre: football_nfl)\n",
                        "12. N12820: cleveland browns baker mayfield rips refs , fines people need to be held accountable (Genre: sports, Subgenre: football_nfl)\n",
                        "13. N846: nfl week 7 awards is this the best photo ever taken of a nfl player ? (Genre: sports, Subgenre: football_nfl)\n",
                        "14. N32565: it s absolutely fair to blame this lions loss on bad officiating (Genre: sports, Subgenre: more_sports)\n",
                        "15. N33394: ravens news 10 18 officiating issues , mvp caliber combatants and more (Genre: sports, Subgenre: football_nfl)\n"
                    ]
                }
            ],
            "source": [
                "valid_news = pd.read_table(\n",
                "    os.path.join(data_path, 'valid', 'news.tsv'),\n",
                "    names=['newid', 'vertical', 'subvertical', 'title', 'abstract', 'url', 'entities in title', 'entities in abstract'],\n",
                "    usecols=['newid', 'vertical', 'subvertical', 'title', 'abstract']\n",
                ")\n",
                "\n",
                "\n",
                "\n",
                "article_index = 5  # Choose a random validation article\n",
                "\n",
                "print(\"\\nTesting on Validation Data:\")\n",
                "print(f\"ID: {valid_news.iloc[article_index]['newid']}\")\n",
                "print(f\"Title: {valid_news.iloc[article_index]['title']}\")\n",
                "print(f\"Genre: {valid_news.iloc[article_index]['vertical']}\")\n",
                "print(f\"Subgenre: {valid_news.iloc[article_index]['subvertical']}\\n\")\n",
                "\n",
                "# Get recommendations based on the validation article\n",
                "recommended_articles = get_recommendations(article_index, top_n=15)\n",
                "\n",
                "print(\"Recommended Articles:\\n\")\n",
                "for idx, article in enumerate(recommended_articles, start=1):\n",
                "    print(f\"{idx}. {article}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "User U80234 previously read:\n",
                        "N55189 N46039 N51741 N53234 N11276 N264 N40716 N28088 N43955 N6616 N47686 N63573 N38895 N30924 N35671\n",
                        "\n",
                        "Recommended articles:\n",
                        "1. N9056: this is what queen elizabeth is doing about the prince william prince harry feud (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "2. N60671: prince charles teared up when prince william talked about succeeding him (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "3. N38133: the cutest photos of royal children and their beloved nannies from prince george to the queen (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "4. N18530: all the photos of prince charles s trip to japan for emperor naruhito s enthronement ceremony (Genre: lifestyle, Subgenre: lifestyleroyals)\n",
                        "5. N63823: prince charles hit by one of the most incredible art hoaxes in royal history (Genre: lifestyle, Subgenre: lifestyleroyals)\n"
                    ]
                }
            ],
            "source": [
                "# Load validation impressions (assuming they exist)\n",
                "valid_behaviors = pd.read_table(\n",
                "    os.path.join(data_path, 'valid', 'behaviors.tsv'),\n",
                "    names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
                ")\n",
                "\n",
                "# Extract a sample user's history\n",
                "sample_user = valid_behaviors.iloc[0]\n",
                "\n",
                "print(f\"User {sample_user['user_id']} previously read:\")\n",
                "print(sample_user['history'])\n",
                "\n",
                "print(\"\\nRecommended articles:\")\n",
                "recommended_articles = get_recommendations(0, top_n=5)\n",
                "for idx, article in enumerate(recommended_articles, start=1):\n",
                "    print(f\"{idx}. {article}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#tmpdir.cleanup()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "\\[1\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
