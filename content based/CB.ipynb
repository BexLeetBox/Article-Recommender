{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68b063617afebd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**1) Build Article Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a150111e148ae0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:18:54.947322900Z",
     "start_time": "2025-03-11T16:18:49.419214900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news_df with shape: (101527, 7)\n",
      "Loaded 42007 entity embeddings.\n",
      "Created embeddings for 89224 articles out of 101527.\n",
      "Saved article embeddings to: article_vectors.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 1) Load the news data\n",
    "def load_news_data(news_file):\n",
    "    \"\"\"\n",
    "    Loads the MIND 'news.tsv' file into a DataFrame with columns:\n",
    "      [news_id, category, subcategory, title, abstract, url, title_entities, abstract_entities]\n",
    "    Returns a DataFrame indexed by news_id.\n",
    "    \"\"\"\n",
    "    cols = ['news_id', 'category', 'subcategory', 'title', 'abstract',\n",
    "            'url', 'title_entities', 'abstract_entities']\n",
    "    news_df = pd.read_csv(news_file, sep='\\t', header=None, names=cols)\n",
    "    news_df.set_index('news_id', inplace=True)\n",
    "    return news_df\n",
    "\n",
    "# 2) Parse the JSON-like entity columns\n",
    "\n",
    "def parse_entity_list(entity_str):\n",
    "    \"\"\"\n",
    "    Given a JSON-like string of entity objects (from 'title_entities' or 'abstract_entities'),\n",
    "    extract the 'WikidataId' fields into a list of IDs.\n",
    "    Example of entity_str:\n",
    "        '[{\"Label\":\"Skin tag\",\"WikidataId\":\"Q12345\",\"Type\":\"C\"}, ...]'\n",
    "    If parsing fails or empty, returns [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(entity_str)\n",
    "        wikidata_ids = [obj['WikidataId'] for obj in data if 'WikidataId' in obj]\n",
    "        return wikidata_ids\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# 3) Load entity embeddings\n",
    "\n",
    "def load_entity_embeddings(embedding_file):\n",
    "    \"\"\"\n",
    "    Loads 'entity_embedding.vec' into a dict: {WikidataId -> embedding (np.array)}.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entity_id = parts[0]            # e.g. 'Q12345'\n",
    "            vector_vals = [float(x) for x in parts[1:]]\n",
    "            embeddings[entity_id] = np.array(vector_vals, dtype=np.float32)\n",
    "    return embeddings\n",
    "\n",
    "# 4) Build per-article embeddings\n",
    "\n",
    "def build_article_embeddings(news_df, embeddings, use_abstract=False):\n",
    "    \"\"\"\n",
    "    For each article in 'news_df', gather all WikidataIds from 'title_entities'\n",
    "    (and optionally 'abstract_entities'), look them up in 'embeddings',\n",
    "    and average them to create a single (d-dimensional) vector.\n",
    "    Returns a dict: { news_id -> np.array (d,) }.\n",
    "    \"\"\"\n",
    "    article_vectors = {}\n",
    "\n",
    "    for news_id, row in news_df.iterrows():\n",
    "        # Parse title entities\n",
    "        title_ids = parse_entity_list(row['title_entities'])\n",
    "        # Optional: parse abstract entities for more coverage\n",
    "        if use_abstract:\n",
    "            abstract_ids = parse_entity_list(row['abstract_entities'])\n",
    "            entity_ids = title_ids + abstract_ids\n",
    "        else:\n",
    "            entity_ids = title_ids\n",
    "\n",
    "        # Gather embeddings\n",
    "        valid_vectors = [embeddings[eid] for eid in entity_ids if eid in embeddings]\n",
    "\n",
    "        if valid_vectors:\n",
    "            article_vectors[news_id] = np.mean(valid_vectors, axis=0)\n",
    "        # else, skip or handle articles with no entities\n",
    "\n",
    "    return article_vectors\n",
    "\n",
    "\n",
    "\n",
    "dataset_dir = '../MINDlarge_train'\n",
    "news_file = os.path.join(dataset_dir, 'news.tsv')\n",
    "embedding_file = os.path.join(dataset_dir, 'entity_embedding.vec')\n",
    "news_df = load_news_data(news_file)\n",
    "embeddings = load_entity_embeddings(embedding_file)\n",
    "article_vectors = build_article_embeddings(news_df, embeddings, use_abstract=True)\n",
    "\n",
    "print(f\"Loaded news_df with shape: {news_df.shape}\")\n",
    "print(f\"Loaded {len(embeddings)} entity embeddings.\")\n",
    "print(f\"Created embeddings for {len(article_vectors)} articles out of {len(news_df)}.\")\n",
    "\n",
    "# (Optional) Save article_vectors to pickle\n",
    "output_file = 'article_vectors.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    " pickle.dump(article_vectors, f)\n",
    "print(f\"Saved article embeddings to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628591e15cf94444",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**2) Cluster Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f14cac28f9e0b7c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:19:10.916204200Z",
     "start_time": "2025-03-11T16:18:54.949831800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with embeddings: 89224\n",
      "Embedding array shape: (89224, 100)\n",
      "Cluster label array shape: (89224,)\n",
      "Saved cluster assignments for 89224 articles.\n",
      "K-Means model also saved to 'kmeans_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the article embeddings from pickle\n",
    "with open('article_vectors.pkl', 'rb') as f:\n",
    "    article_vectors = pickle.load(f)\n",
    "\n",
    "print(\"Number of articles with embeddings:\", len(article_vectors))\n",
    "\n",
    "# Prepare data for clustering: shape (num_articles, embedding_dim)\n",
    "article_ids = list(article_vectors.keys())\n",
    "X = np.stack([article_vectors[a] for a in article_ids], axis=0)\n",
    "print(\"Embedding array shape:\", X.shape)\n",
    "\n",
    "# Choose your K (number of clusters)\n",
    "num_clusters = 50\n",
    "\n",
    "# Fit K-Means (or another clustering algorithm)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# The cluster label for each article\n",
    "labels = kmeans.labels_  # shape: (num_articles,)\n",
    "print(\"Cluster label array shape:\", labels.shape)\n",
    "\n",
    "# Build a dictionary: article_id -> cluster_label\n",
    "cluster_assignments = {\n",
    "    article_id: int(cluster_label)\n",
    "    for article_id, cluster_label in zip(article_ids, labels)\n",
    "}\n",
    "\n",
    "# (Optional) Save the cluster assignments and the KMeans model\n",
    "with open('cluster_assignments.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_assignments, f)\n",
    "\n",
    "with open('kmeans_model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "print(f\"Saved cluster assignments for {len(cluster_assignments)} articles.\")\n",
    "print(\"K-Means model also saved to 'kmeans_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb5bcec48f3945",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**3)Build User Profiles (Train Set)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97a57dc5990cd000",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:21:08.043411100Z",
     "start_time": "2025-03-11T16:19:10.917207100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_28868\\2315440084.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train_behaviors_df = pd.read_csv('../MINDlarge_train/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n",
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_28868\\2315440084.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test_behaviors_df = pd.read_csv('../MINDlarge_dev/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built profiles for 694981 users.\n",
      "Saved user profiles to 'user_profiles.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "cols = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "train_behaviors_df = pd.read_csv('../MINDlarge_train/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n",
    "test_behaviors_df = pd.read_csv('../MINDlarge_dev/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n",
    "\n",
    "# Load cluster assignments (article_id -> cluster_label)\n",
    "with open('cluster_assignments.pkl', 'rb') as f:\n",
    "    cluster_assignments = pickle.load(f)\n",
    "\n",
    "# If you know how many clusters you used (e.g. K=50):\n",
    "num_clusters = 50\n",
    "\n",
    "def build_user_profiles(behaviors_df, cluster_assignments, num_clusters=50):\n",
    "    \"\"\"\n",
    "    Build user profiles as probability distributions over cluster assignments.\n",
    "    Returns a dict: { user_id -> np.array of shape (num_clusters,) }.\n",
    "    \"\"\"\n",
    "    # user_clusters[u] = np.array to accumulate counts per cluster\n",
    "    user_clusters = defaultdict(lambda: np.zeros(num_clusters, dtype=np.float32))\n",
    "\n",
    "    for row in behaviors_df.itertuples(index=False):\n",
    "        # row has: impression_id, user_id, time, history, impressions\n",
    "        # (Adjust if your column names differ.)\n",
    "        user_id = row.user_id\n",
    "        history_str = row.history  # e.g. \"N12345 N23456\"\n",
    "\n",
    "        if not isinstance(history_str, str):\n",
    "            continue  # skip if empty or NaN\n",
    "\n",
    "        clicked_articles = history_str.split()  # list of article IDs\n",
    "\n",
    "        for art_id in clicked_articles:\n",
    "            if art_id in cluster_assignments:\n",
    "                cluster_label = cluster_assignments[art_id]\n",
    "                user_clusters[user_id][cluster_label] += 1\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    user_profiles = {}\n",
    "    for u, counts in user_clusters.items():\n",
    "        total_clicked = counts.sum()\n",
    "        if total_clicked > 0:\n",
    "            user_profiles[u] = counts / total_clicked\n",
    "        else:\n",
    "            # If user never clicked anything (edge case), store zeros\n",
    "            user_profiles[u] = counts\n",
    "\n",
    "    return user_profiles\n",
    "\n",
    "# Actually build the profiles\n",
    "user_profiles = build_user_profiles(train_behaviors_df, cluster_assignments, num_clusters=num_clusters)\n",
    "print(f\"Built profiles for {len(user_profiles)} users.\")\n",
    "\n",
    "# Save profiles to a pickle\n",
    "with open('user_profiles.pkl', 'wb') as f:\n",
    "    pickle.dump(user_profiles, f)\n",
    "\n",
    "print(\"Saved user profiles to 'user_profiles.pkl'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb87107019c051",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Generate Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6c03d59507a4620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:21:09.478428400Z",
     "start_time": "2025-03-11T16:21:08.046406600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user U13740: ['N118326', 'N86270', 'N100928', 'N948', 'N102409']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# from sklearn.metrics.pairwise import cosine_similarity  # optional if you want similarity-based ranking\n",
    "\n",
    "# 1) Load required data\n",
    "with open('user_profiles.pkl', 'rb') as f:\n",
    "    user_profiles = pickle.load(f)  # { user_id -> np.array of shape (num_clusters,) }\n",
    "\n",
    "with open('cluster_assignments.pkl', 'rb') as f:\n",
    "    cluster_assignments = pickle.load(f)  # { article_id -> cluster_label }\n",
    "\n",
    "# (Optional) load article embeddings if you want advanced ranking:\n",
    "with open('article_vectors.pkl', 'rb') as f:\n",
    "    article_vectors = pickle.load(f)  # { article_id -> np.array(dim,) }\n",
    "\n",
    "# 2) Build a reverse map: cluster -> all articles in that cluster\n",
    "cluster_to_articles = defaultdict(list)\n",
    "for art_id, cluster_label in cluster_assignments.items():\n",
    "    cluster_to_articles[cluster_label].append(art_id)\n",
    "\n",
    "def recommend_for_user(\n",
    "    user_id,\n",
    "    user_profiles,\n",
    "    cluster_to_articles,\n",
    "    article_vectors=None,\n",
    "    top_clusters=2,\n",
    "    top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend top_k articles for the given user by:\n",
    "      1) Selecting the top_clusters with the highest probability in user_profiles.\n",
    "      2) Gathering all articles from those clusters.\n",
    "      3) (Optional) Ranking them by similarity to user's embedding (or any heuristic).\n",
    "    \"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        # Cold start or invalid user\n",
    "        return []\n",
    "\n",
    "    # 1) Identify top clusters for this user\n",
    "    distribution = user_profiles[user_id]  # shape: (num_clusters,)\n",
    "    # sort clusters by descending probability\n",
    "    cluster_indices = np.argsort(distribution)[::-1]\n",
    "    selected_clusters = cluster_indices[:top_clusters]\n",
    "\n",
    "    # 2) Gather candidate articles from these clusters\n",
    "    candidate_articles = []\n",
    "    for c_idx in selected_clusters:\n",
    "        candidate_articles.extend(cluster_to_articles[c_idx])\n",
    "    candidate_articles = list(set(candidate_articles))  # remove duplicates if clusters overlap\n",
    "\n",
    "    # 3) (Optional) Rank candidates by similarity to user’s embedding\n",
    "    # One approach is to build a \"user embedding\" by weighting cluster centroids or\n",
    "    # by averaging the embeddings of the user’s clicked articles. For now, let's skip advanced ranking.\n",
    "\n",
    "    # Quick naive approach: just pick the first top_k\n",
    "    # If you have an advanced approach, you'd do something like:\n",
    "    \"\"\"\n",
    "    user_embed = ... # e.g., an average of clicked article vectors\n",
    "    sims = []\n",
    "    for art_id in candidate_articles:\n",
    "        if art_id in article_vectors:\n",
    "            art_vec = article_vectors[art_id]\n",
    "            # compute cosine similarity\n",
    "            similarity_val = np.dot(user_embed, art_vec) / (np.linalg.norm(user_embed)*np.linalg.norm(art_vec))\n",
    "            sims.append((art_id, similarity_val))\n",
    "    # sort by similarity desc\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    recommended = [x[0] for x in sims[:top_k]]\n",
    "    \"\"\"\n",
    "    # But for this minimal example, we do no ranking\n",
    "    recommended = candidate_articles[:top_k]\n",
    "\n",
    "    return recommended\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "test_user_id = 'U13740'  # or pick any user from user_profiles\n",
    "recommendations = recommend_for_user(test_user_id, user_profiles, cluster_to_articles, article_vectors, top_clusters=2, top_k=5)\n",
    "print(f\"Recommendations for user {test_user_id}:\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29efc52fe0a3342c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:21:09.732016700Z",
     "start_time": "2025-03-11T16:21:09.481432900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def get_article_title(news_df, article_id):\n",
    "    \"\"\"Return the title for a given article_id, or a fallback message if not found.\"\"\"\n",
    "    if article_id in news_df.index:\n",
    "        return news_df.loc[article_id, 'title']\n",
    "    return \"[Title not found]\"\n",
    "\n",
    "def recommend_for_user(\n",
    "    user_id,\n",
    "    user_profiles,\n",
    "    cluster_to_articles,\n",
    "    news_df,\n",
    "    train_behaviors_df,\n",
    "    article_vectors,\n",
    "    top_clusters=2,\n",
    "    top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend top_k articles for the given user by:\n",
    "      1) Selecting the top_clusters with the highest probability in user_profiles.\n",
    "      2) Gathering all articles from those clusters.\n",
    "      3) Building a user embedding by averaging the embeddings of the user's clicked articles.\n",
    "      4) Ranking those candidate articles by cosine similarity to the user embedding.\n",
    "      5) Returning the top_k.\n",
    "    Also prints some user history for context, plus recommended article titles.\n",
    "    \"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        print(f\"No profile found for user {user_id}. Returning empty list.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    \n",
    "    # ----------------------------------\n",
    "    # Identify top clusters for this user\n",
    "    # ----------------------------------\n",
    "    distribution = user_profiles[user_id]  # shape: (num_clusters,)\n",
    "    cluster_indices = np.argsort(distribution)[::-1]\n",
    "    selected_clusters = cluster_indices[:top_clusters]\n",
    "\n",
    "    # Gather candidate articles from these clusters\n",
    "    candidate_articles = []\n",
    "    for c_idx in selected_clusters:\n",
    "        candidate_articles.extend(cluster_to_articles[c_idx])\n",
    "    candidate_articles = list(set(candidate_articles))  # remove duplicates\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Build the user's embedding\n",
    "    # ----------------------------------\n",
    "    # For a better user embedding, gather *all* the user's clicked articles from train_behaviors_df\n",
    "    full_clicked = []\n",
    "    user_rows = train_behaviors_df[train_behaviors_df['user_id'] == user_id]\n",
    "    for row_idx, row in user_rows.iterrows():\n",
    "        history_str = row['history']\n",
    "        if isinstance(history_str, str):\n",
    "            full_clicked.extend(history_str.split())\n",
    "\n",
    "    # Filter only the articles that have an embedding\n",
    "    valid_clicked_embeddings = [\n",
    "        article_vectors[a] for a in full_clicked if a in article_vectors\n",
    "    ]\n",
    "\n",
    "    if not valid_clicked_embeddings:\n",
    "        print(f\"User {user_id} has no article embeddings in clicked history. Cannot rank by similarity.\")\n",
    "        # fallback: just return naive\n",
    "        recommended = candidate_articles[:top_k]\n",
    "        return recommended\n",
    "\n",
    "    user_embedding = np.mean(valid_clicked_embeddings, axis=0)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Rank candidates by cosine similarity\n",
    "    # ----------------------------------\n",
    "    scored_candidates = []\n",
    "    for art_id in candidate_articles:\n",
    "        # skip if article has no embedding\n",
    "        if art_id not in article_vectors:\n",
    "            continue\n",
    "\n",
    "        art_vec = article_vectors[art_id]\n",
    "        # Cosine similarity\n",
    "        sim = np.dot(user_embedding, art_vec) / (norm(user_embedding)*norm(art_vec))\n",
    "        scored_candidates.append((art_id, sim))\n",
    "\n",
    "    # Sort by similarity desc\n",
    "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # pick top_k\n",
    "    recommended = [x[0] for x in scored_candidates[:top_k]]\n",
    "\n",
    "\n",
    "\n",
    "    return recommended\n",
    "\n",
    "# Example usage:\n",
    "# We'll assume:\n",
    "#  - user_profiles, cluster_to_articles loaded from pickle\n",
    "#  - train_behaviors_df is training set DataFrame\n",
    "#  - news_df is articles DataFrame, indexed by news_id\n",
    "\n",
    "test_user_id = 'U72339'\n",
    "#test_user_id = 'U13740'  # example user\n",
    "recommendations = recommend_for_user(\n",
    "    test_user_id,\n",
    "    user_profiles,\n",
    "    cluster_to_articles,\n",
    "    news_df,\n",
    "    train_behaviors_df,\n",
    "    article_vectors,\n",
    "    top_clusters=2,\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26e05e69773cd0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Evaluation and Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "930176a1da00d7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:21:09.739951900Z",
     "start_time": "2025-03-11T16:21:09.735076200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ndcg_k(ranked_list, ground_truth, k=5):\n",
    "    \"\"\"\n",
    "    ranked_list: list of article IDs in recommended order\n",
    "    ground_truth: set or list of the clicked (relevant) article IDs\n",
    "    k: rank cutoff\n",
    "    \n",
    "    Returns NDCG at K\n",
    "    \"\"\"\n",
    "    # DCG\n",
    "    dcg = 0.0\n",
    "    for i, art_id in enumerate(ranked_list[:k]):\n",
    "        if art_id in ground_truth:\n",
    "            # relevance is 1 if clicked, 0 otherwise\n",
    "            dcg += 1.0 / np.log2(i + 2)  # index i is 0-based, so position is i+1; we use i+2 for the log\n",
    "    \n",
    "    # IDCG (Ideal DCG) if all relevant items are at the top\n",
    "    # if the user clicked m items, the best possible DCG at k is the sum of 1/log2 of positions, for min(m, k) times\n",
    "    ideal_count = min(len(ground_truth), k)\n",
    "    idcg = 0.0\n",
    "    for i in range(ideal_count):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def mrr_k(ranked_list, ground_truth, k=5):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank @ K\n",
    "    Returns 0 if no relevant article is found in the top k.\n",
    "    \"\"\"\n",
    "    for i, art_id in enumerate(ranked_list[:k]):\n",
    "        if art_id in ground_truth:\n",
    "            return 1.0 / (i + 1)  # i is 0-based\n",
    "    return 0.0\n",
    "\n",
    "def auc_score(ranked_list, ground_truth):\n",
    "    \"\"\"\n",
    "    A simple AUC approach: for each pair (clicked vs. not-clicked) in the ranking,\n",
    "    check ordering. We'll treat the index in 'ranked_list' as the predicted rank\n",
    "    (lower index => higher predicted relevance).\n",
    "    \n",
    "    This is O(n^2) for the length of ranked_list. For large lists, a more efficient approach is recommended.\n",
    "    \"\"\"\n",
    "    # Convert ground_truth to a set for quick membership test\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    # Build a list of (article_id, label, rank)\n",
    "    labeled_ranked = []\n",
    "    for rank, art_id in enumerate(ranked_list):\n",
    "        label = 1 if art_id in ground_truth_set else 0\n",
    "        labeled_ranked.append((art_id, label, rank))\n",
    "    \n",
    "    # We'll count pairwise (clicked vs not clicked)\n",
    "    n_pairs = 0\n",
    "    n_correct = 0\n",
    "    for i in range(len(labeled_ranked)):\n",
    "        for j in range(i+1, len(labeled_ranked)):\n",
    "            label_i = labeled_ranked[i][1]\n",
    "            label_j = labeled_ranked[j][1]\n",
    "            if label_i != label_j:\n",
    "                n_pairs += 1\n",
    "                # if item i is relevant (label=1) and item j is not (label=0), that is correct if i < j\n",
    "                if label_i == 1 and i < j:\n",
    "                    n_correct += 1\n",
    "                # if item j is relevant and i is not, correct if j < i\n",
    "                if label_j == 1 and j < i:\n",
    "                    n_correct += 1\n",
    "    if n_pairs == 0:\n",
    "        return 0.0\n",
    "    return n_correct / n_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84af1491820f51d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:21:10.443165300Z",
     "start_time": "2025-03-11T16:21:09.737952300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in both train and test: 216778\n"
     ]
    }
   ],
   "source": [
    "train_users = set(train_behaviors_df['user_id'].unique())\n",
    "test_users = set(test_behaviors_df['user_id'].unique())\n",
    "\n",
    "eval_users = train_users & test_users\n",
    "print(f\"Users in both train and test: {len(eval_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfdcaef71eea33",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No profile found for user U222267. Returning empty list.\n",
      "No profile found for user U98049. Returning empty list.\n",
      "No profile found for user U319486. Returning empty list.\n",
      "No profile found for user U190294. Returning empty list.\n",
      "No profile found for user U5961. Returning empty list.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # or just import tqdm if you're not in a notebook\n",
    "\n",
    "def parse_impressions(impressions_str):\n",
    "    \"\"\"\n",
    "    Given something like 'N123-1 N456-0 N789-1', \n",
    "    return (all_article_ids, clicked_articles).\n",
    "    \"\"\"\n",
    "    items = impressions_str.split()\n",
    "    all_ids = []\n",
    "    clicked = []\n",
    "    for x in items:\n",
    "        article_id, label_str = x.split('-')\n",
    "        all_ids.append(article_id)\n",
    "        if label_str == '1':\n",
    "            clicked.append(article_id)\n",
    "    return all_ids, clicked\n",
    "\n",
    "# Evaluate on each impression\n",
    "\n",
    "all_ndcg = []\n",
    "all_mrr = []\n",
    "all_auc = []\n",
    "k_eval = 5\n",
    "\n",
    "# We'll assume test_behaviors_df is a pandas DataFrame with your test impressions.\n",
    "for row in tqdm(test_behaviors_df.itertuples(index=False), \n",
    "                total=len(test_behaviors_df), \n",
    "                desc=\"Evaluating\"):\n",
    "    user_id = row.user_id\n",
    "    if user_id not in eval_users:\n",
    "        continue\n",
    "    \n",
    "    impressions_str = row.impressions\n",
    "    all_articles, clicked_articles = parse_impressions(impressions_str)\n",
    "\n",
    "    ranked_list = recommend_for_user(\n",
    "        user_id,\n",
    "        user_profiles,\n",
    "        cluster_to_articles,\n",
    "        news_df,\n",
    "        train_behaviors_df,\n",
    "        article_vectors,\n",
    "        top_clusters=2,\n",
    "        top_k=len(all_articles)\n",
    "    )\n",
    "    \n",
    "    ndcg_val = ndcg_k(ranked_list, clicked_articles, k=k_eval)\n",
    "    mrr_val = mrr_k(ranked_list, clicked_articles, k=k_eval)\n",
    "    auc_val = auc_score(ranked_list, clicked_articles)\n",
    "    \n",
    "    all_ndcg.append(ndcg_val)\n",
    "    all_mrr.append(mrr_val)\n",
    "    all_auc.append(auc_val)\n",
    "\n",
    "mean_ndcg = np.mean(all_ndcg) if all_ndcg else 0.0\n",
    "mean_mrr = np.mean(all_mrr) if all_mrr else 0.0\n",
    "mean_auc = np.mean(all_auc) if all_auc else 0.0\n",
    "\n",
    "print(f\"NDCG@{k_eval}: {mean_ndcg:.4f}\")\n",
    "print(f\"MRR@{k_eval}:  {mean_mrr:.4f}\")\n",
    "print(f\"AUC:           {mean_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb3f7128ed045",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
