{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68b063617afebd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**1) Build Article Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a150111e148ae0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:16.217264200Z",
     "start_time": "2025-04-04T16:58:13.585153200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news_df with shape: (51282, 7)\n",
      "Loaded 26904 entity embeddings.\n",
      "Created embeddings for 44262 articles out of 51282.\n",
      "Saved article embeddings to: article_vectors.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 1) Load the news data\n",
    "def load_news_data(news_file):\n",
    "    \"\"\"\n",
    "    Loads the MIND 'news.tsv' file into a DataFrame with columns:\n",
    "      [news_id, category, subcategory, title, abstract, url, title_entities, abstract_entities]\n",
    "    Returns a DataFrame indexed by news_id.\n",
    "    \"\"\"\n",
    "    cols = ['news_id', 'category', 'subcategory', 'title', 'abstract',\n",
    "            'url', 'title_entities', 'abstract_entities']\n",
    "    news_df = pd.read_csv(news_file, sep='\\t', header=None, names=cols)\n",
    "    news_df.set_index('news_id', inplace=True)\n",
    "    return news_df\n",
    "\n",
    "# 2) Parse the JSON-like entity columns\n",
    "\n",
    "def parse_entity_list(entity_str):\n",
    "    \"\"\"\n",
    "    Given a JSON-like string of entity objects (from 'title_entities' or 'abstract_entities'),\n",
    "    extract the 'WikidataId' fields into a list of IDs.\n",
    "    Example of entity_str:\n",
    "        '[{\"Label\":\"Skin tag\",\"WikidataId\":\"Q12345\",\"Type\":\"C\"}, ...]'\n",
    "    If parsing fails or empty, returns [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(entity_str)\n",
    "        wikidata_ids = [obj['WikidataId'] for obj in data if 'WikidataId' in obj]\n",
    "        return wikidata_ids\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# 3) Load entity embeddings\n",
    "\n",
    "def load_entity_embeddings(embedding_file):\n",
    "    \"\"\"\n",
    "    Loads 'entity_embedding.vec' into a dict: {WikidataId -> embedding (np.array)}.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entity_id = parts[0]            # e.g. 'Q12345'\n",
    "            vector_vals = [float(x) for x in parts[1:]]\n",
    "            embeddings[entity_id] = np.array(vector_vals, dtype=np.float32)\n",
    "    return embeddings\n",
    "\n",
    "# 4) Build per-article embeddings\n",
    "\n",
    "def build_article_embeddings(news_df, embeddings, use_abstract=False):\n",
    "    \"\"\"\n",
    "    For each article in 'news_df', gather all WikidataIds from 'title_entities'\n",
    "    (and optionally 'abstract_entities'), look them up in 'embeddings',\n",
    "    and average them to create a single (d-dimensional) vector.\n",
    "    Returns a dict: { news_id -> np.array (d,) }.\n",
    "    \"\"\"\n",
    "    article_vectors = {}\n",
    "\n",
    "    for news_id, row in news_df.iterrows():\n",
    "        # Parse title entities\n",
    "        title_ids = parse_entity_list(row['title_entities'])\n",
    "        # Optional: parse abstract entities for more coverage\n",
    "        if use_abstract:\n",
    "            abstract_ids = parse_entity_list(row['abstract_entities'])\n",
    "            entity_ids = title_ids + abstract_ids\n",
    "        else:\n",
    "            entity_ids = title_ids\n",
    "\n",
    "        # Gather embeddings\n",
    "        valid_vectors = [embeddings[eid] for eid in entity_ids if eid in embeddings]\n",
    "\n",
    "        if valid_vectors:\n",
    "            article_vectors[news_id] = np.mean(valid_vectors, axis=0)\n",
    "        # else, skip or handle articles with no entities\n",
    "\n",
    "    return article_vectors\n",
    "\n",
    "# Example usage in Jupyter:\n",
    "# (Adjust the file paths as needed for your environment)\n",
    "\n",
    "dataset_dir = '../MINDsmall_train'\n",
    "news_file = os.path.join(dataset_dir, 'news.tsv')\n",
    "embedding_file = os.path.join(dataset_dir, 'entity_embedding.vec')\n",
    "news_df = load_news_data(news_file)\n",
    "embeddings = load_entity_embeddings(embedding_file)\n",
    "article_vectors = build_article_embeddings(news_df, embeddings, use_abstract=True)\n",
    "\n",
    "print(f\"Loaded news_df with shape: {news_df.shape}\")\n",
    "print(f\"Loaded {len(embeddings)} entity embeddings.\")\n",
    "print(f\"Created embeddings for {len(article_vectors)} articles out of {len(news_df)}.\")\n",
    "\n",
    "# (Optional) Save article_vectors to pickle\n",
    "output_file = 'article_vectors.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    " pickle.dump(article_vectors, f)\n",
    "print(f\"Saved article embeddings to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628591e15cf94444",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**2) Cluster Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14cac28f9e0b7c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:22.391850100Z",
     "start_time": "2025-04-04T16:58:16.219774100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with embeddings: 44262\n",
      "Embedding array shape: (44262, 100)\n",
      "Cluster label array shape: (44262,)\n",
      "Saved cluster assignments for 44262 articles.\n",
      "K-Means model also saved to 'kmeans_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the article embeddings from pickle\n",
    "with open('article_vectors.pkl', 'rb') as f:\n",
    "    article_vectors = pickle.load(f)\n",
    "\n",
    "print(\"Number of articles with embeddings:\", len(article_vectors))\n",
    "\n",
    "# Prepare data for clustering: shape (num_articles, embedding_dim)\n",
    "article_ids = list(article_vectors.keys())\n",
    "X = np.stack([article_vectors[a] for a in article_ids], axis=0)\n",
    "print(\"Embedding array shape:\", X.shape)\n",
    "\n",
    "# Choose your K (number of clusters)\n",
    "num_clusters = 50\n",
    "\n",
    "# Fit K-Means (or another clustering algorithm)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# The cluster label for each article\n",
    "labels = kmeans.labels_  # shape: (num_articles,)\n",
    "print(\"Cluster label array shape:\", labels.shape)\n",
    "\n",
    "# Build a dictionary: article_id -> cluster_label\n",
    "cluster_assignments = {\n",
    "    article_id: int(cluster_label)\n",
    "    for article_id, cluster_label in zip(article_ids, labels)\n",
    "}\n",
    "\n",
    "# (Optional) Save the cluster assignments and the KMeans model\n",
    "with open('cluster_assignments.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_assignments, f)\n",
    "\n",
    "with open('kmeans_model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "print(f\"Saved cluster assignments for {len(cluster_assignments)} articles.\")\n",
    "print(\"K-Means model also saved to 'kmeans_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb5bcec48f3945",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**3)Build User Profiles (Train Set)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c250a93e7f4fb718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:28.556396600Z",
     "start_time": "2025-04-04T16:58:22.392846800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_38140\\3751932590.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train_behaviors_df = pd.read_csv('../MINDsmall_train/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n",
      "C:\\Users\\Bex\\AppData\\Local\\Temp\\ipykernel_38140\\3751932590.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test_behaviors_df = pd.read_csv('../MINDsmall_dev/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built profiles for 48887 users.\n",
      "Saved user profiles to 'user_profiles.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "cols = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "train_behaviors_df = pd.read_csv('../MINDsmall_train/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n",
    "test_behaviors_df = pd.read_csv('../MINDsmall_dev/behaviors.tsv', sep='\\\\t', header=None, names=cols)\n",
    "\n",
    "# Load cluster assignments (article_id -> cluster_label)\n",
    "with open('cluster_assignments.pkl', 'rb') as f:\n",
    "    cluster_assignments = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def build_user_profiles_embedding_based(behaviors_df, article_vectors):\n",
    "    user_profiles = {}\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        if not isinstance(row[\"history\"], str):\n",
    "            continue\n",
    "        clicked_ids = row[\"history\"].split()\n",
    "        vectors = [article_vectors[aid] for aid in clicked_ids if aid in article_vectors]\n",
    "        if vectors:\n",
    "            user_profiles[user_id] = np.mean(vectors, axis=0)\n",
    "    return user_profiles\n",
    "\n",
    "# Actually build the profiles\n",
    "user_profiles = build_user_profiles_embedding_based(train_behaviors_df, article_vectors)\n",
    "\n",
    "print(f\"Built profiles for {len(user_profiles)} users.\")\n",
    "\n",
    "# Save profiles to a pickle\n",
    "with open('user_profiles.pkl', 'wb') as f:\n",
    "    pickle.dump(user_profiles, f)\n",
    "\n",
    "print(\"Saved user profiles to 'user_profiles.pkl'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c004efd51c411f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Generate Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3893709889ade279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:28.710001100Z",
     "start_time": "2025-04-04T16:58:28.556396600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user U13740: ['N14637', 'N62254', 'N29180', 'N50637', 'N20904']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# from sklearn.metrics.pairwise import cosine_similarity  # optional if you want similarity-based ranking\n",
    "\n",
    "# 1) Load required data\n",
    "with open('user_profiles.pkl', 'rb') as f:\n",
    "    user_profiles = pickle.load(f)  # { user_id -> np.array of shape (num_clusters,) }\n",
    "\n",
    "with open('cluster_assignments.pkl', 'rb') as f:\n",
    "    cluster_assignments = pickle.load(f)  # { article_id -> cluster_label }\n",
    "\n",
    "# (Optional) load article embeddings if you want advanced ranking:\n",
    "with open('article_vectors.pkl', 'rb') as f:\n",
    "    article_vectors = pickle.load(f)  # { article_id -> np.array(dim,) }\n",
    "\n",
    "# 2) Build a reverse map: cluster -> all articles in that cluster\n",
    "cluster_to_articles = defaultdict(list)\n",
    "for art_id, cluster_label in cluster_assignments.items():\n",
    "    cluster_to_articles[cluster_label].append(art_id)\n",
    "\n",
    "def recommend_for_user(\n",
    "    user_id,\n",
    "    user_profiles,\n",
    "    cluster_to_articles,\n",
    "    article_vectors=None,\n",
    "    top_clusters=2,\n",
    "    top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend top_k articles for the given user by:\n",
    "      1) Selecting the top_clusters with the highest probability in user_profiles.\n",
    "      2) Gathering all articles from those clusters.\n",
    "      3) (Optional) Ranking them by similarity to user's embedding (or any heuristic).\n",
    "    \"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        # Cold start or invalid user\n",
    "        return []\n",
    "\n",
    "    # 1) Identify top clusters for this user\n",
    "    distribution = user_profiles[user_id]  # shape: (num_clusters,)\n",
    "    # sort clusters by descending probability\n",
    "    cluster_indices = np.argsort(distribution)[::-1]\n",
    "    selected_clusters = cluster_indices[:top_clusters]\n",
    "\n",
    "    # 2) Gather candidate articles from these clusters\n",
    "    candidate_articles = []\n",
    "    for c_idx in selected_clusters:\n",
    "        candidate_articles.extend(cluster_to_articles[c_idx])\n",
    "    candidate_articles = list(set(candidate_articles))  # remove duplicates if clusters overlap\n",
    "\n",
    "    # 3) (Optional) Rank candidates by similarity to user’s embedding\n",
    "    # One approach is to build a \"user embedding\" by weighting cluster centroids or\n",
    "    # by averaging the embeddings of the user’s clicked articles. For now, let's skip advanced ranking.\n",
    "\n",
    "    # Quick naive approach: just pick the first top_k\n",
    "    # If you have an advanced approach, you'd do something like:\n",
    "    \"\"\"\n",
    "    user_embed = ... # e.g., an average of clicked article vectors\n",
    "    sims = []\n",
    "    for art_id in candidate_articles:\n",
    "        if art_id in article_vectors:\n",
    "            art_vec = article_vectors[art_id]\n",
    "            # compute cosine similarity\n",
    "            similarity_val = np.dot(user_embed, art_vec) / (np.linalg.norm(user_embed)*np.linalg.norm(art_vec))\n",
    "            sims.append((art_id, similarity_val))\n",
    "    # sort by similarity desc\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    recommended = [x[0] for x in sims[:top_k]]\n",
    "    \"\"\"\n",
    "    # But for this minimal example, we do no ranking\n",
    "    recommended = candidate_articles[:top_k]\n",
    "\n",
    "    return recommended\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "test_user_id = 'U13740'  # or pick any user from user_profiles\n",
    "recommendations = recommend_for_user(test_user_id, user_profiles, cluster_to_articles, article_vectors, top_clusters=2, top_k=5)\n",
    "print(f\"Recommendations for user {test_user_id}:\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7fa6ece622e31ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:28.749900100Z",
     "start_time": "2025-04-04T16:58:28.713506300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def get_article_title(news_df, article_id):\n",
    "    \"\"\"Return the title for a given article_id, or a fallback message if not found.\"\"\"\n",
    "    if article_id in news_df.index:\n",
    "        return news_df.loc[article_id, 'title']\n",
    "    return \"[Title not found]\"\n",
    "\n",
    "def recommend_for_user(\n",
    "    user_id,\n",
    "    user_profiles,\n",
    "    cluster_to_articles,\n",
    "    news_df,\n",
    "    train_behaviors_df,\n",
    "    article_vectors,\n",
    "    top_clusters=2,\n",
    "    top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend top_k articles for the given user by:\n",
    "      1) Selecting the top_clusters with the highest probability in user_profiles.\n",
    "      2) Gathering all articles from those clusters.\n",
    "      3) Building a user embedding by averaging the embeddings of the user's clicked articles.\n",
    "      4) Ranking those candidate articles by cosine similarity to the user embedding.\n",
    "      5) Returning the top_k.\n",
    "    Also prints some user history for context, plus recommended article titles.\n",
    "    \"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        print(f\"No profile found for user {user_id}. Returning empty list.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "     # ----------------------------------\n",
    "    # Print some of the user's history\n",
    "    # ----------------------------------\n",
    "    user_rows = train_behaviors_df[train_behaviors_df['user_id'] == user_id]\n",
    "    if user_rows.empty:\n",
    "        print(f\"No behavior records found for user {user_id}.\")\n",
    "    else:\n",
    "        first_row = user_rows.iloc[0]\n",
    "        history_str = first_row['history']\n",
    "        if isinstance(history_str, str):\n",
    "            clicked_articles = history_str.split()\n",
    "            #print(f\"User {user_id} clicked {len(clicked_articles)} articles. Showing up to 15 titles:\")\n",
    "            for art_id in clicked_articles[:15]:\n",
    "                title = news_df.loc[art_id, 'title'] if art_id in news_df.index else '[Title not found]'\n",
    "                #print(f\" - {art_id}: {title}\")\n",
    "        else:\n",
    "            print(f\"User {user_id} has no clicked articles in the first row record.\")\n",
    "    \n",
    "    # ----------------------------------\n",
    "    # Identify top clusters for this user\n",
    "    # ----------------------------------\n",
    "    distribution = user_profiles[user_id]  # shape: (num_clusters,)\n",
    "    cluster_indices = np.argsort(distribution)[::-1]\n",
    "    selected_clusters = cluster_indices[:top_clusters]\n",
    "\n",
    "    # Gather candidate articles from these clusters\n",
    "    candidate_articles = []\n",
    "    for c_idx in selected_clusters:\n",
    "        candidate_articles.extend(cluster_to_articles[c_idx])\n",
    "    candidate_articles = list(set(candidate_articles))  # remove duplicates\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Build the user's embedding\n",
    "    # ----------------------------------\n",
    "    # For a better user embedding, gather *all* the user's clicked articles from train_behaviors_df\n",
    "    full_clicked = []\n",
    "    user_rows = train_behaviors_df[train_behaviors_df['user_id'] == user_id]\n",
    "    for row_idx, row in user_rows.iterrows():\n",
    "        history_str = row['history']\n",
    "        if isinstance(history_str, str):\n",
    "            full_clicked.extend(history_str.split())\n",
    "\n",
    "    # Filter only the articles that have an embedding\n",
    "    valid_clicked_embeddings = [\n",
    "        article_vectors[a] for a in full_clicked if a in article_vectors\n",
    "    ]\n",
    "\n",
    "    if not valid_clicked_embeddings:\n",
    "        #print(f\"User {user_id} has no article embeddings in clicked history. Cannot rank by similarity.\")\n",
    "        # fallback: just return naive\n",
    "        recommended = candidate_articles[:top_k]\n",
    "        return recommended\n",
    "\n",
    "    user_embedding = np.mean(valid_clicked_embeddings, axis=0)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Rank candidates by cosine similarity\n",
    "    # ----------------------------------\n",
    "    scored_candidates = []\n",
    "    for art_id in candidate_articles:\n",
    "        # skip if article has no embedding\n",
    "        if art_id not in article_vectors:\n",
    "            continue\n",
    "\n",
    "        art_vec = article_vectors[art_id]\n",
    "        # Cosine similarity\n",
    "        sim = np.dot(user_embedding, art_vec) / (norm(user_embedding)*norm(art_vec))\n",
    "        scored_candidates.append((art_id, sim))\n",
    "\n",
    "    # Sort by similarity desc\n",
    "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # pick top_k\n",
    "    recommended = [x[0] for x in scored_candidates[:top_k]]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        # ----------------------------------\n",
    "        # Print recommended article titles\n",
    "        # ----------------------------------\n",
    "        print(f\"\\nRecommended articles for user {user_id}:\")\n",
    "        for art_id in recommended:\n",
    "            if art_id in news_df.index:\n",
    "                print(f\" - {art_id}: {news_df.loc[art_id, 'title']}\")\n",
    "            else:\n",
    "                print(f\" - {art_id}: [Title not found]\")\n",
    "    \"\"\"\n",
    "    return recommended\n",
    "\n",
    "# Example usage:\n",
    "# We'll assume:\n",
    "#  - user_profiles, cluster_to_articles loaded from pickle\n",
    "#  - train_behaviors_df is training set DataFrame\n",
    "#  - news_df is articles DataFrame, indexed by news_id\n",
    "\n",
    "test_user_id = 'U72339'\n",
    "#test_user_id = 'U13740'  # example user\n",
    "recommendations = recommend_for_user(\n",
    "    test_user_id,\n",
    "    user_profiles,\n",
    "    cluster_to_articles,\n",
    "    news_df,\n",
    "    train_behaviors_df,\n",
    "    article_vectors,\n",
    "    top_clusters=2,\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa16f107a551ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Evaluation and Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "138faf611a7ff0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:28.752010400Z",
     "start_time": "2025-04-04T16:58:28.746685800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ndcg_k(ranked_list, ground_truth, k=5):\n",
    "    \"\"\"\n",
    "    ranked_list: list of article IDs in recommended order\n",
    "    ground_truth: set or list of the clicked (relevant) article IDs\n",
    "    k: rank cutoff\n",
    "    \n",
    "    Returns NDCG at K\n",
    "    \"\"\"\n",
    "    # DCG\n",
    "    dcg = 0.0\n",
    "    for i, art_id in enumerate(ranked_list[:k]):\n",
    "        if art_id in ground_truth:\n",
    "            # relevance is 1 if clicked, 0 otherwise\n",
    "            dcg += 1.0 / np.log2(i + 2)  # index i is 0-based, so position is i+1; we use i+2 for the log\n",
    "    \n",
    "    # IDCG (Ideal DCG) if all relevant items are at the top\n",
    "    # if the user clicked m items, the best possible DCG at k is the sum of 1/log2 of positions, for min(m, k) times\n",
    "    ideal_count = min(len(ground_truth), k)\n",
    "    idcg = 0.0\n",
    "    for i in range(ideal_count):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def mrr_k(ranked_list, ground_truth, k=5):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank @ K\n",
    "    Returns 0 if no relevant article is found in the top k.\n",
    "    \"\"\"\n",
    "    for i, art_id in enumerate(ranked_list[:k]):\n",
    "        if art_id in ground_truth:\n",
    "            return 1.0 / (i + 1)  # i is 0-based\n",
    "    return 0.0\n",
    "\n",
    "def auc_score(ranked_list, ground_truth):\n",
    "    \"\"\"\n",
    "    A simple AUC approach: for each pair (clicked vs. not-clicked) in the ranking,\n",
    "    check ordering. We'll treat the index in 'ranked_list' as the predicted rank\n",
    "    (lower index => higher predicted relevance).\n",
    "    \n",
    "    This is O(n^2) for the length of ranked_list. For large lists, a more efficient approach is recommended.\n",
    "    \"\"\"\n",
    "    # Convert ground_truth to a set for quick membership test\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    # Build a list of (article_id, label, rank)\n",
    "    labeled_ranked = []\n",
    "    for rank, art_id in enumerate(ranked_list):\n",
    "        label = 1 if art_id in ground_truth_set else 0\n",
    "        labeled_ranked.append((art_id, label, rank))\n",
    "    \n",
    "    # We'll count pairwise (clicked vs not clicked)\n",
    "    n_pairs = 0\n",
    "    n_correct = 0\n",
    "    for i in range(len(labeled_ranked)):\n",
    "        for j in range(i+1, len(labeled_ranked)):\n",
    "            label_i = labeled_ranked[i][1]\n",
    "            label_j = labeled_ranked[j][1]\n",
    "            if label_i != label_j:\n",
    "                n_pairs += 1\n",
    "                # if item i is relevant (label=1) and item j is not (label=0), that is correct if i < j\n",
    "                if label_i == 1 and i < j:\n",
    "                    n_correct += 1\n",
    "                # if item j is relevant and i is not, correct if j < i\n",
    "                if label_j == 1 and j < i:\n",
    "                    n_correct += 1\n",
    "    if n_pairs == 0:\n",
    "        return 0.0\n",
    "    return n_correct / n_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb349eae07b1c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T16:58:28.769810400Z",
     "start_time": "2025-04-04T16:58:28.749900100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in both train and test: 5943\n"
     ]
    }
   ],
   "source": [
    "train_users = set(train_behaviors_df['user_id'].unique())\n",
    "test_users = set(test_behaviors_df['user_id'].unique())\n",
    "\n",
    "eval_users = train_users & test_users\n",
    "print(f\"Users in both train and test: {len(eval_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2274e15b3948bf58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T17:00:44.728078300Z",
     "start_time": "2025-04-04T16:58:28.769810400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No profile found for user U17949. Returning empty list.\n",
      "No profile found for user U4780. Returning empty list.\n",
      "No profile found for user U17949. Returning empty list.\n",
      "Evaluation results on test (over users in both train & test):\n",
      "NDCG@10: 0.00013462\n",
      "MRR@10:  0.00009511\n",
      "AUC:           0.00053390\n"
     ]
    }
   ],
   "source": [
    "def filter_users_with_min_clicks(behaviors_file, min_clicks=10):\n",
    "    \"\"\"\n",
    "    Filters users with at least `min_clicks` in their click history.\n",
    "    Returns a DataFrame of filtered user interactions.\n",
    "    \"\"\"\n",
    "    # Load behaviors file\n",
    "    cols = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "    behaviors_df = pd.read_csv(behaviors_file, sep='\\t', header=None, names=cols)\n",
    "\n",
    "    # Count clicks in the 'history' column (split by space)\n",
    "    behaviors_df['click_count'] = behaviors_df['history'].apply(lambda x: len(x.split()) if pd.notnull(x) else 0)\n",
    "\n",
    "    # Filter users with at least `min_clicks`\n",
    "    filtered_users = behaviors_df[behaviors_df['click_count'] >= min_clicks]\n",
    "    return filtered_users\n",
    "\n",
    "\n",
    "\n",
    "def parse_impressions(impressions_str):\n",
    "    \"\"\"\n",
    "    Given something like 'N123-1 N456-0 N789-1', \n",
    "    return (all_article_ids, clicked_articles).\n",
    "    \"\"\"\n",
    "    items = impressions_str.split()\n",
    "    all_ids = []\n",
    "    clicked = []\n",
    "    for x in items:\n",
    "        article_id, label_str = x.split('-')\n",
    "        all_ids.append(article_id)\n",
    "        if label_str == '1':\n",
    "            clicked.append(article_id)\n",
    "    return all_ids, clicked\n",
    "\n",
    "\n",
    "\n",
    "behaviors_file = os.path.join(dataset_dir, 'behaviors.tsv')\n",
    "filtered_users_df = filter_users_with_min_clicks(behaviors_file, min_clicks=5)\n",
    "\n",
    "# Filter test behaviors to include only users with sufficient history\n",
    "test_behaviors_df = test_behaviors_df[test_behaviors_df['user_id'].isin(filtered_users_df['user_id'])]\n",
    "\n",
    "\n",
    "# Evaluate on each impression\n",
    "all_ndcg = []\n",
    "all_mrr = []\n",
    "all_auc = []\n",
    "\n",
    "k_eval = 10  # We'll evaluate at k=5 for NDCG & MRR\n",
    "\n",
    "for row in test_behaviors_df.itertuples(index=False):\n",
    "    user_id = row.user_id\n",
    "    if user_id not in eval_users:\n",
    "        continue  # skip users not in train-test overlap\n",
    "    \n",
    "    impressions_str = row.impressions\n",
    "    all_articles, clicked_articles = parse_impressions(impressions_str)\n",
    "\n",
    "    # Generate a ranked list from your recommender\n",
    "    # e.g. use the same function from step 4 (with ranking)\n",
    "    ranked_list = recommend_for_user(\n",
    "        user_id,\n",
    "        user_profiles,\n",
    "        cluster_to_articles,\n",
    "        news_df,\n",
    "        train_behaviors_df,\n",
    "        article_vectors,\n",
    "        top_clusters=2,\n",
    "        top_k=len(all_articles)  # We can rank at least the same size as impressions\n",
    "    )\n",
    "\n",
    "    # Evaluate vs ground truth\n",
    "    ndcg_val = ndcg_k(ranked_list, clicked_articles, k=k_eval)\n",
    "    mrr_val = mrr_k(ranked_list, clicked_articles, k=k_eval)\n",
    "    auc_val = auc_score(ranked_list, clicked_articles)\n",
    "    \n",
    "    all_ndcg.append(ndcg_val)\n",
    "    all_mrr.append(mrr_val)\n",
    "    all_auc.append(auc_val)\n",
    "\n",
    "# Final average\n",
    "mean_ndcg = np.mean(all_ndcg) if all_ndcg else 0.0\n",
    "mean_mrr = np.mean(all_mrr) if all_mrr else 0.0\n",
    "mean_auc = np.mean(all_auc) if all_auc else 0.0\n",
    "\n",
    "print(f\"Evaluation results on test (over users in both train & test):\")\n",
    "print(f\"NDCG@{k_eval}: {mean_ndcg:.8f}\")\n",
    "print(f\"MRR@{k_eval}:  {mean_mrr:.8f}\")\n",
    "print(f\"AUC:           {mean_auc:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a7646fb521330a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T17:03:02.848065600Z",
     "start_time": "2025-04-04T17:00:44.730129400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mContentBasedRecommender\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, article_vectors, user_profiles):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils.evaluation import evaluate_model\n",
    "\n",
    "\n",
    "class ContentBasedRecommender:\n",
    "    def __init__(self, article_vectors, user_profiles):\n",
    "        self.article_vectors = article_vectors\n",
    "        self.user_profiles = user_profiles\n",
    "\n",
    "        # Use your good logic from recommend_for_user\n",
    "        self.recommend = lambda user_id, N: recommend_for_user(\n",
    "            user_id,\n",
    "            self.user_profiles,\n",
    "            cluster_to_articles,\n",
    "            news_df,\n",
    "            train_behaviors_df,\n",
    "            self.article_vectors,\n",
    "            top_clusters=1,\n",
    "            top_k=N\n",
    "        )\n",
    "\n",
    "# Initialize the recommender\n",
    "recommender = ContentBasedRecommender(article_vectors, user_profiles)\n",
    "\n",
    "# Use the filtered test behaviors for evaluation\n",
    "K = 5  # Set the value of K for evaluation\n",
    "avg_ndcg, avg_auc, avg_mrr = evaluate_model(recommender, test_behaviors_df, K)\n",
    "\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"NDCG@{K}: {avg_ndcg:.8f}\")\n",
    "print(f\"AUC@{K}: {avg_auc:.8f}\")\n",
    "print(f\"MRR@{K}: {avg_mrr:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5e0f2c65ece0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T17:03:02.852124800Z",
     "start_time": "2025-04-04T17:03:02.848065600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
