{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<i>Adapted from Recommenders ALS example</i>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Running ALS on MIND (with PySpark)\n",
                "\n",
                "Matrix factorization by [ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS) (Alternating Least Squares) is a well known collaborative filtering algorithm.\n",
                "\n",
                "This notebook provides an example of how to utilize and evaluate ALS PySpark ML (DataFrame-based API) implementation, meant for large-scale distributed datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\n",
                        "Spark version: 3.5.4\n"
                    ]
                }
            ],
            "source": [
                "import warnings\n",
                "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import pyspark\n",
                "from pyspark.ml.recommendation import ALS\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.types import StructType, StructField\n",
                "from pyspark.sql.types import StringType, IntegerType\n",
                "\n",
                "from recommenders.utils.timer import Timer\n",
                "from recommenders.datasets.spark_splitters import spark_random_split\n",
                "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
                "from recommenders.utils.spark_utils import start_or_get_spark\n",
                "\n",
                "from tempfile import TemporaryDirectory\n",
                "from recommenders.datasets.mind import download_mind\n",
                "from recommenders.datasets.download_utils import unzip_file\n",
                "\n",
                "print(f\"System version: {sys.version}\")\n",
                "print(\"Spark version: {}\".format(pyspark.__version__))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Set the default parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# top k items to recommend\n",
                "TOP_K = 10\n",
                "\n",
                "# MIND sizes: \"demo\", \"small\", or \"large\"\n",
                "mind_type = 'demo'\n",
                "\n",
                "# Column names for the dataset\n",
                "COL_USER = \"user_id\"\n",
                "COL_ITEM = \"news_id\"\n",
                "COL_RATING = \"rating\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 0. Set up Spark context & directory\n",
                "\n",
                "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/04/06 09:29:45 WARN Utils: Your hostname, sondre-ThinkPad-E580 resolves to a loopback address: 127.0.1.1; using 10.21.36.87 instead (on interface enx6c02e0d7834b)\n",
                        "25/04/06 09:29:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "25/04/06 09:29:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/04/06 09:30:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
                    ]
                }
            ],
            "source": [
                "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
                "# set up a giant single executor with many threads and specify memory cap\n",
                "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
                "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 17.0k/17.0k [00:08<00:00, 1.99kKB/s]\n",
                        "100%|██████████| 9.84k/9.84k [00:03<00:00, 2.78kKB/s]\n"
                    ]
                }
            ],
            "source": [
                "# Setup data storage location\n",
                "\n",
                "tmpdir = TemporaryDirectory()\n",
                "data_path = tmpdir.name\n",
                "train_zip, valid_zip = download_mind(size=mind_type, dest_path=data_path)\n",
                "unzip_file(train_zip, os.path.join(data_path, 'train'), clean_zip_file=False)\n",
                "unzip_file(valid_zip, os.path.join(data_path, 'valid'), clean_zip_file=False)\n",
                "train_behaviors_path = os.path.join(data_path, \"train\", \"behaviors.tsv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Download the MIND dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-------+-------+------+-----+-----+\n",
                        "|news_id|user_id|rating|count|count|\n",
                        "+-------+-------+------+-----+-----+\n",
                        "|  13390|  82271|     0|  162| 1893|\n",
                        "|   7180|  82271|     0|  162|  725|\n",
                        "|  20785|  82271|     0|  162|  881|\n",
                        "|   6937|  82271|     0|  162| 1269|\n",
                        "|  15776|  82271|     0|  162|  604|\n",
                        "|  25810|  82271|     0|  162|  391|\n",
                        "|  20820|  82271|     0|  162|  590|\n",
                        "|  27294|  82271|     0|  162|  513|\n",
                        "|  18835|  82271|     0|  162|  864|\n",
                        "|  16945|  82271|     0|  162| 1305|\n",
                        "|   7410|  82271|     0|  162|   31|\n",
                        "|  23967|  82271|     0|  162|  937|\n",
                        "|  22679|  82271|     0|  162|  255|\n",
                        "|  20532|  82271|     0|  162|  484|\n",
                        "|  26651|  82271|     0|  162| 1523|\n",
                        "|  22078|  82271|     0|  162| 1486|\n",
                        "|   4098|  82271|     0|  162|  708|\n",
                        "|  16473|  82271|     0|  162| 1195|\n",
                        "|  13841|  82271|     0|  162| 1387|\n",
                        "|  15660|  82271|     0|  162|  639|\n",
                        "+-------+-------+------+-----+-----+\n",
                        "only showing top 20 rows\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 13:=============================>                            (2 + 2) / 4]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+------+\n",
                        "|rating| count|\n",
                        "+------+------+\n",
                        "|     1| 33306|\n",
                        "|     0|790435|\n",
                        "+------+------+\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "# Schema for behaviors.tsv\n",
                "schema = StructType([\n",
                "    StructField(\"impression_id\", StringType(), True),  # Ignored for ALS\n",
                "    StructField(COL_USER, StringType(), True),  # Will be converted later\n",
                "    StructField(\"timestamp\", StringType(), True),  # Convert to long if needed\n",
                "    StructField(\"history\", StringType(), True),  # List of past clicked news\n",
                "    StructField(\"impressions\", StringType(), True)  # Needs to be split into news_id + rating\n",
                "])\n",
                "\n",
                "# Load raw behaviors.tsv\n",
                "data = (\n",
                "    spark.read.option(\"sep\", \"\\t\").option(\"header\", \"false\")\n",
                "    .schema(schema)\n",
                "    .csv(train_behaviors_path)\n",
                ")\n",
                "\n",
                "# Split impressions column (\"n4-1 n5-0 n6-1\") into separate rows\n",
                "data = data.withColumn(\"impressions\", F.explode(F.split(F.col(\"impressions\"), \" \")))\n",
                "\n",
                "# Extract news_id and click status (e.g., \"n4-1\" → news_id=\"n4\", rating=1)\n",
                "data = data.withColumn(COL_ITEM, F.split(F.col(\"impressions\"), \"-\")[0])\n",
                "data = data.withColumn(COL_RATING, F.split(F.col(\"impressions\"), \"-\")[1].cast(IntegerType()))\n",
                "\n",
                "# Convert user_id and news_id to integers (ALS requires numeric IDs)\n",
                "from pyspark.sql.functions import regexp_extract, col\n",
                "from pyspark.sql.types import IntegerType\n",
                "\n",
                "# Extract numeric part from IDs like \"U123\" or \"N456\"\n",
                "data = data.withColumn(COL_USER, regexp_extract(col(COL_USER), r\"\\d+\", 0).cast(IntegerType()))\n",
                "data = data.withColumn(COL_ITEM, regexp_extract(col(COL_ITEM), r\"\\d+\", 0).cast(IntegerType()))\n",
                "\n",
                "# Drop unnecessary columns\n",
                "data = data.select(COL_USER, COL_ITEM, COL_RATING)\n",
                "\n",
                "# Count and remove articles / users with few interactions\n",
                "user_counts = data.groupBy(COL_USER).count().filter(F.col(\"count\") >= 10)\n",
                "news_counts = data.groupBy(COL_ITEM).count().filter(F.col(\"count\") >= 10)\n",
                "\n",
                "data = data.join(user_counts, \"user_id\").join(news_counts, \"news_id\")\n",
                "\n",
                "\n",
                "# Show transformed data\n",
                "data.show()\n",
                "data.groupBy(COL_RATING).count().show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Split the data using the Spark random splitter provided in utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/04/06 09:40:07 WARN CacheManager: Asked to cache already cached data.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "N train 618004\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/04/06 09:40:07 WARN CacheManager: Asked to cache already cached data.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "N test 205737\n"
                    ]
                }
            ],
            "source": [
                "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
                "print (\"N train\", train.cache().count())\n",
                "print (\"N test\", test.cache().count())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Train the ALS model on the training data, and get the top-k recommendations for our testing data\n",
                "\n",
                "To article interactions movie ratings, we use the rating data in the training set as users' explicit feedback."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "header = {\n",
                "    \"userCol\": COL_USER,\n",
                "    \"itemCol\": COL_ITEM,\n",
                "    \"ratingCol\": COL_RATING,\n",
                "}\n",
                "\n",
                "\n",
                "als = ALS(\n",
                "    rank=50,\n",
                "    maxIter=15,\n",
                "    implicitPrefs=True,\n",
                "    regParam=0.01,\n",
                "    coldStartStrategy='drop',\n",
                "    nonnegative=True,\n",
                "    seed=42,\n",
                "    alpha=45,\n",
                "    **header\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/04/06 09:40:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 23.569571076000102 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "with Timer() as train_time:\n",
                "    model = als.fit(train)\n",
                "\n",
                "print(f\"Took {train_time.interval} seconds for training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the movie recommendation use case, recommending movies that have been rated by the users do not make sense. Therefore, the rated movies are removed from the recommended items.\n",
                "\n",
                "In order to achieve this, we recommend all movies to all users, and then remove the user-movie pairs that exist in the training dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/04/06 09:40:50 WARN Column: Constructing trivially true equals predicate, 'user_id#33 = user_id#33'. Perhaps you need to use aliases.\n",
                        "25/04/06 09:40:50 WARN Column: Constructing trivially true equals predicate, 'news_id#41 = news_id#41'. Perhaps you need to use aliases.\n",
                        "[Stage 724:=================================================>   (185 + 8) / 200]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 278.7115564999999 seconds for prediction.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "with Timer() as test_time:\n",
                "\n",
                "    # Get the cross join of all user-item pairs and score them.\n",
                "    users = train.select(COL_USER).distinct()\n",
                "    items = train.select(COL_ITEM).distinct()\n",
                "    user_item = users.crossJoin(items)\n",
                "    dfs_pred = model.transform(user_item)\n",
                "\n",
                "    # Remove seen items.\n",
                "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
                "        train.alias(\"train\"),\n",
                "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
                "        how='outer'\n",
                "    )\n",
                "\n",
                "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
                "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
                "\n",
                "    # In Spark, transformations are lazy evaluation\n",
                "    # Use an action to force execute and measure the test time \n",
                "    top_all.cache().count()\n",
                "\n",
                "print(f\"Took {test_time.interval} seconds for prediction.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-------+-------+------------+\n",
                        "|user_id|news_id|  prediction|\n",
                        "+-------+-------+------------+\n",
                        "|     17|   1538|  0.15820055|\n",
                        "|     17|   3172|         0.0|\n",
                        "|     17|   4803|0.0012495205|\n",
                        "|     17|  11644|  0.01418963|\n",
                        "|     17|  11821|         0.0|\n",
                        "|     17|  16109|0.0051098084|\n",
                        "|     17|  21503|         0.0|\n",
                        "|     17|  24459|         0.0|\n",
                        "|     17|  24768|         0.0|\n",
                        "|     17|  25311|         0.0|\n",
                        "|     17|  26103|0.0034035179|\n",
                        "|     17|  27784|  0.04035833|\n",
                        "|     22|    947|  0.02954644|\n",
                        "|     22|   1810|         0.0|\n",
                        "|     22|   2157| 0.019164255|\n",
                        "|     22|   2740|         0.0|\n",
                        "|     22|   5823|         0.0|\n",
                        "|     22|  10213|         0.0|\n",
                        "|     22|  16362|         0.0|\n",
                        "|     22|  16810|         0.0|\n",
                        "+-------+-------+------------+\n",
                        "only showing top 20 rows\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "top_all.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Evaluate how well ALS performs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
                "                                    relevancy_method=\"top_k\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 765:======================================>                  (6 + 3) / 9]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS\n",
                        "Top K:\t10\n",
                        "MAP:\t0.024824\n",
                        "NDCG:\t0.063141\n",
                        "Precision@K:\t0.056736\n",
                        "Recall@K:\t0.019012\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "print(\"Model:\\tALS\",\n",
                "      \"Top K:\\t%d\" % rank_eval.k,\n",
                "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
                "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
                "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
                "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluate rating prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 840:==============================================>      (174 + 8) / 200]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-------+-------+------+-----+-----+------------+\n",
                        "|news_id|user_id|rating|count|count|  prediction|\n",
                        "+-------+-------+------+-----+-----+------------+\n",
                        "|    294|  15790|     0|  262| 1421|0.0065011675|\n",
                        "|    425|  38758|     1|   61| 2066|         0.0|\n",
                        "|    431|  43935|     1|  335|   97|         0.0|\n",
                        "|    533|  21700|     0|  133|   82| 0.052334305|\n",
                        "|    657|  43935|     0|  335|  101|         0.0|\n",
                        "|    712|  12027|     0|  235|  152|0.0014256807|\n",
                        "|    824|  78120|     0|  258|  928|0.0021753795|\n",
                        "|   1132|  12027|     0|  235| 1966|  0.78306013|\n",
                        "|   1138|  12027|     0|  235|  107|  0.08100026|\n",
                        "|   1636|  78120|     0|  258|  309|  0.11024747|\n",
                        "|   2022|  12027|     0|  235|  303| 8.478404E-4|\n",
                        "|   2581|  21700|     0|  133|   14|         0.0|\n",
                        "|   2770|  78120|     0|  258| 1010|0.0037514006|\n",
                        "|   2864|  19079|     1|   81|  944| 0.038738415|\n",
                        "|   2864|  57039|     0|   30|  944|         0.0|\n",
                        "|   2916|  12027|     0|  235|  666|         0.0|\n",
                        "|   2974|  19079|     0|   81| 1310| 0.019264665|\n",
                        "|   3174|  78120|     0|  258|   22|         0.0|\n",
                        "|   3277|  12027|     0|  235|  124|  0.06833042|\n",
                        "|   3305|  78120|     0|  258|  567| 1.829498E-5|\n",
                        "+-------+-------+------+-----+-----+------------+\n",
                        "only showing top 20 rows\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "# Generate predicted ratings.\n",
                "prediction = model.transform(test)\n",
                "prediction.cache().show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 865:============================================>        (167 + 8) / 200]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS rating prediction\n",
                        "RMSE:\t0.317041\n",
                        "MAE:\t0.167151\n",
                        "Explained variance:\t-1.418830\n",
                        "R squared:\t-1.719973\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
                "\n",
                "print(\"Model:\\tALS rating prediction\",\n",
                "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
                "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
                "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
                "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cleanup spark instance and clear temp directory\n",
                "spark.stop()\n",
                "tmpdir.cleanup()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Changes over time\n",
                "\n",
                "#### Attempt 0 - no modifications\n",
                "\n",
                "Model:\tALS\n",
                "Top K:\t10\n",
                "MAP:\t0.000041\n",
                "NDCG:\t0.000069\n",
                "Precision@K:\t0.000020\n",
                "Recall@K:\t0.000041\n",
                "\n",
                "Model:\tALS rating prediction\n",
                "RMSE:\t0.194057\n",
                "MAE:\t0.047414\n",
                "Explained variance:\t0.019782\n",
                "R squared:\t-0.002237\n",
                "\n",
                "These extremely low metrics might be due to the dataset mostly being made up of non-interactions. Possible changes:\n",
                "- Remove articles with low engagement\n",
                "- Tune hyperparameters: increase rank, decrease regParam, and change alpha\n",
                "- Convert data to implicit feedback\n",
                "\n",
                "#### Attempt 1 - adding atmpt. 0 suggestions\n",
                "\n",
                "Model:\tALS\n",
                "Top K:\t10\n",
                "MAP:\t0.026334\n",
                "NDCG:\t0.066722\n",
                "Precision@K:\t0.058610\n",
                "Recall@K:\t0.020850\n",
                "\n",
                "Model:\tALS rating prediction\n",
                "RMSE:\t0.317204\n",
                "MAE:\t0.167511\n",
                "Explained variance:\t-1.369951\n",
                "R squared:\t-1.660865\n",
                "\n",
                "Much better, but still very low. Might attempt a different method before continuing. \n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
