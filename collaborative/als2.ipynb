{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\n",
      "Pandas version: 2.2.3\n",
      "PySpark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n",
    "from recommenders.tuning.parameter_sweep import generate_param_grid\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 10\n",
    "MAX_ITER = 15\n",
    "REG_PARAM = 0.05\n",
    "\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing of behavior dataset\n",
    "\n",
    "import re\n",
    "\n",
    "def aggregateRating(x): \n",
    "  items = x.split(\" \")\n",
    "  clicked = re.findall(r\"\\w+-1\", x)\n",
    "  return len(items) // len(clicked)\n",
    "\n",
    "def getClicked(x):\n",
    "  return re.findall(r\"(\\w+)-1\", x)\n",
    "\n",
    "def str2int(x):\n",
    "  m = re.search(r\"\\d+\", x)\n",
    "  if not m:\n",
    "    raise Exception(\"didnt find number in the index\")\n",
    "\n",
    "  return int(m.group())\n",
    "\n",
    "def load_impression(path):\n",
    "\n",
    "  impressions = pd.read_csv(\n",
    "      path,\n",
    "      sep='\\t',\n",
    "      header=None,\n",
    "      names=[\"impressionId\", \"userId\", \"time\", \"history\", \"impressions\"]\n",
    "    )\n",
    "\n",
    "  impressions = impressions.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  impressions[\"rating\"] = impressions[\"impressions\"].apply(aggregateRating)\n",
    "  impressions[\"newsId\"] = impressions[\"impressions\"].apply(getClicked)\n",
    "  impressions = impressions.explode(\"newsId\").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "  # Convert ids to int for als\n",
    "  impressions[\"user\"] = impressions[\"userId\"].apply(str2int)\n",
    "  impressions[\"item\"] = impressions[\"newsId\"].apply(str2int)\n",
    "\n",
    "  impressions = impressions[[\"user\", \"item\", \"rating\"]]\n",
    "\n",
    "  return impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 231530 entries, 0 to 231529\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   user    231530 non-null  int64\n",
      " 1   item    231530 non-null  int64\n",
      " 2   rating  231530 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 5.3 MB\n",
      "None\n",
      "\n",
      "test\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 107968 entries, 0 to 107967\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   user    107968 non-null  int64\n",
      " 1   item    107968 non-null  int64\n",
      " 2   rating  107968 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 2.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "impression_train = load_impression(\"../MIND/train/behaviors.tsv\")\n",
    "impression_test = load_impression(\"../MIND/test/behaviors.tsv\")\n",
    "\n",
    "print(\"train\")\n",
    "print(impression_train.info())\n",
    "print(\"\\ntest\\n\")\n",
    "print(impression_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 10:58:46 WARN Utils: Your hostname, sondre-ThinkPad-E580 resolves to a loopback address: 127.0.1.1; using 10.252.38.200 instead (on interface wlp5s0)\n",
      "25/04/03 10:58:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/03 10:58:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/03 10:59:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/04/03 10:59:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import SparkSession if you haven't already\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession if not already created\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Recommendation System\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert your pandas DataFrame to a Spark DataFrame\n",
    "impressions_spark_train = spark.createDataFrame(impression_train)\n",
    "impressions_spark_test = spark.createDataFrame(impression_test)\n",
    "\n",
    "# Now use the Spark DataFrame with ALS\n",
    "als = ALS(\n",
    "    maxIter=MAX_ITER,\n",
    "    rank=RANK,\n",
    "    regParam=REG_PARAM,\n",
    "    userCol=\"user\",\n",
    "    itemCol=\"item\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "# Use the Spark DataFrame, not the pandas DataFrame\n",
    "model = als.fit(impressions_spark_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'item', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(impressions_spark_train).drop(\"rating\")\n",
    "\n",
    "print(prediction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score = 35.43585540187076\n",
      "MAE score = 24.79759323332045\n",
      "R2 score = -0.1470159775856399\n",
      "Explained variance score = -0.14337300674306608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "evaluations = SparkRatingEvaluation(\n",
    "  impressions_spark_test,\n",
    "  prediction,\n",
    "  col_user=\"user\",\n",
    "  col_item=\"item\",\n",
    "  col_rating=\"rating\",\n",
    "  col_prediction=\"prediction\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"RMSE score = {evaluations.rmse()}\",\n",
    "    f\"MAE score = {evaluations.mae()}\",\n",
    "    f\"R2 score = {evaluations.rsquared()}\",\n",
    "    f\"Explained variance score = {evaluations.exp_var()}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                (0 + 1) / 1][Stage 999:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|user| item|prediction|\n",
      "+----+-----+----------+\n",
      "|  19| 2869|-57.652687|\n",
      "|  19| 4034|  36.51317|\n",
      "|  19| 4637| 2.4916067|\n",
      "|  19| 5497| 11.431451|\n",
      "|  19| 5582| 2.9806218|\n",
      "|  19| 8509|-5.3680344|\n",
      "|  19| 8855| -58.02987|\n",
      "|  19| 9392|  5.129809|\n",
      "|  19| 9464|  -8.60764|\n",
      "|  19|10754| -68.82276|\n",
      "|  19|10929|  4.175267|\n",
      "|  19|11830| 16.848124|\n",
      "|  19|13299|-30.541786|\n",
      "|  19|14329|  -23.0581|\n",
      "|  19|14436|  45.06962|\n",
      "|  19|14977| 44.303524|\n",
      "|  19|15279|  9.623946|\n",
      "|  19|16804| 46.107254|\n",
      "|  19|17312| 1.7360251|\n",
      "|  19|18144| 14.178882|\n",
      "+----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "COL_USER = \"user\"\n",
    "COL_ITEM = \"item\"\n",
    "\n",
    "dfs_train = impressions_spark_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the cross join of all user-item pairs and score them.\n",
    "users = dfs_train.select(COL_USER).distinct().limit(1000)  # Limit to 1000 users\n",
    "items = dfs_train.select(COL_ITEM).distinct().limit(1000)  # Limit to 1000 items\n",
    "user_item = users.crossJoin(items)\n",
    "dfs_pred = model.transform(user_item)\n",
    "\n",
    "\n",
    "# Remove seen items.\n",
    "dfs_train = dfs_train.selectExpr(\"user as train_user\", \"item as train_item\", \"rating as train_rating\")\n",
    "\n",
    "dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "    dfs_train.alias(\"train\"),\n",
    "    (col(\"pred.user\") == col(\"train.train_user\")) & (col(\"pred.item\") == col(\"train.train_item\")),\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "dfs_pred_final = dfs_pred_exclude_train.filter(col(\"train.train_rating\").isNull()) \\\n",
    "    .selectExpr(\"pred.user\", \"pred.item\", \"pred.prediction\")\n",
    "\n",
    "dfs_pred_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                (0 + 1) / 1][Stage 1101:>               (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@k = 0.0\n",
      "Recall@k = 0.0\n",
      "NDCG@k = 0.0\n",
      "Mean average precision = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "dfs_test = impressions_spark_test\n",
    "COL_RATING=\"rating\"\n",
    "COL_PREDICTION=\"prediction\"\n",
    "\n",
    "K = 10\n",
    "\n",
    "evaluations = SparkRankingEvaluation(\n",
    "    dfs_test, \n",
    "    dfs_pred_final,\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=K\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Precision@k = {evaluations.precision_at_k()}\",\n",
    "    f\"Recall@k = {evaluations.recall_at_k()}\",\n",
    "    f\"NDCG@k = {evaluations.ndcg_at_k()}\",\n",
    "    f\"Mean average precision = {evaluations.map_at_k()}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
