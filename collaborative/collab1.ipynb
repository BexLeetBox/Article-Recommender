{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_impressions = pd.read_csv(\n",
    "    \"../MIND/train/behaviors.tsv\",\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=[\"impressionId\", \"userId\", \"time\", \"history\", \"impressions\"],\n",
    "    usecols=[\"userId\", \"impressions\"]\n",
    "  )\n",
    "\n",
    "raw_news = pd.read_csv(\n",
    "  \"../MIND/train/news.tsv\",\n",
    "  sep=\"\\t\", \n",
    "  header=None,\n",
    "  names=[\"newsId\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"titleEntities\", \"abstractEntities\"],\n",
    "  usecols=[\"newsId\", \"title\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   userId  100000 non-null  object\n",
      " 1   newsId  100000 non-null  object\n",
      " 2   click   100000 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.3+ MB\n",
      "None\n",
      "   userId  newsId  click\n",
      "0  U13740  N55689      1\n",
      "1  U91836  N17059      1\n",
      "2  U73700  N23814      1\n",
      "3  U34670  N49685      1\n",
      "4   U8125   N8400      1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "impressions = raw_impressions\n",
    "\n",
    "def _split_clicked(x):\n",
    "  return re.findall(r\"(\\w+)-1\", x)\n",
    "\n",
    "impressions = impressions.dropna()\n",
    "\n",
    "impressions[\"newsId\"] = impressions[\"impressions\"].apply(_split_clicked)\n",
    "impressions = impressions.explode(\"newsId\").reset_index()\n",
    "impressions = impressions.head(100_000)\n",
    "\n",
    "impressions[\"click\"] = 1\n",
    "\n",
    "impressions = impressions[[\"userId\", \"newsId\", \"click\"]]\n",
    "print(impressions.info())\n",
    "print(impressions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34324, 5617)\n"
     ]
    }
   ],
   "source": [
    "from collab_utils import create_x\n",
    "\n",
    "X, uid2index, nid2index, index2uid, index2nid = create_x(impressions)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Item-based CF\n",
    "item_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "item_knn.fit(X.T)\n",
    "\n",
    "# User-based CF\n",
    "user_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "user_knn.fit(X)\n",
    "\n",
    "def recommend_items_item_based(user_index, X, item_knn, n_recommendations=5):\n",
    "    # Find items this user has interacted with\n",
    "    user_items = X[user_index].nonzero()[1]\n",
    "    \n",
    "    # Initialize recommendation scores\n",
    "    recommendations = np.zeros(X.shape[1])\n",
    "    \n",
    "    for item in user_items:\n",
    "        # Find similar items\n",
    "        distances, indices = item_knn.kneighbors(\n",
    "            X.T[item].reshape(1, -1), \n",
    "            n_neighbors=11  # Including itself\n",
    "        )\n",
    "        \n",
    "        # Convert distances to similarities and remove the item itself\n",
    "        similarities = 1 - distances.flatten()\n",
    "        similar_items = indices.flatten()\n",
    "        \n",
    "        # Skip the first one as it's the item itself\n",
    "        for i, similar_item in enumerate(similar_items[1:]):\n",
    "            # Weight by similarity\n",
    "            recommendations[similar_item] += similarities[i+1]\n",
    "    \n",
    "    # Filter out items the user has already interacted with\n",
    "    recommendations[user_items] = 0\n",
    "    \n",
    "    # Get top recommendations\n",
    "    top_recommendations = recommendations.argsort()[-n_recommendations:][::-1]\n",
    "    \n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Number of latent factors\n",
    "n_factors = 50\n",
    "\n",
    "# Initialize NMF model\n",
    "nmf_model = NMF(n_components=n_factors, init='random', random_state=42)\n",
    "\n",
    "# Fit the model to the user-item matrix\n",
    "user_factors = nmf_model.fit_transform(X)\n",
    "item_factors = nmf_model.components_\n",
    "\n",
    "# Generate recommendations for a user\n",
    "def recommend_items_mf(user_index, user_factors, item_factors, X, n_recommendations=5):\n",
    "    # Predict ratings for all items\n",
    "    user_vector = user_factors[user_index].reshape(1, -1)\n",
    "    predicted_ratings = np.dot(user_vector, item_factors)\n",
    "    \n",
    "    # Set already interacted items to zero\n",
    "    user_items = X[user_index].nonzero()[1]\n",
    "    predicted_ratings[0, user_items] = 0\n",
    "    \n",
    "    # Get top recommendations\n",
    "    top_recommendations = predicted_ratings[0].argsort()[-n_recommendations:][::-1]\n",
    "    \n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NMF model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondre/projects/anbefaling/recommenders/venv/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitting time: 57.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|██████████| 13761/13761 [00:36<00:00, 379.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 851\n",
      "Mean Precision@5: 0.0018\n",
      "Mean Recall@5: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_recommender_optimized(interactions_df, method='mf', test_size=0.2, k=5):\n",
    "    train_df, test_df = train_test_split(interactions_df, test_size=test_size, random_state=42)\n",
    "\n",
    "    train_matrix, _, _, _, _ = create_x(train_df)\n",
    "    \n",
    "    test_users = np.array([uid2index[uid] for uid in test_df['userId']])\n",
    "    test_items = np.array([nid2index[nid] for nid in test_df['newsId']])\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Fitting NMF model\")\n",
    "\n",
    "    nmf_model = NMF(n_components=50, init='random', random_state=42)\n",
    "    user_factors = nmf_model.fit_transform(train_matrix)\n",
    "    item_factors = nmf_model.components_\n",
    "\n",
    "    print(f\"Model fitting time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    precision_at_k = []\n",
    "    recall_at_k = []\n",
    "\n",
    "    unique_test_users = np.unique(test_users)\n",
    "\n",
    "    skipped = 0\n",
    "    \n",
    "    for user_idx in tqdm(unique_test_users, desc=\"Evaluating users\"):\n",
    "        try: \n",
    "            recommendations = recommend_items_mf(user_idx, user_factors, item_factors, train_matrix, n_recommendations=k)\n",
    "        except: \n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        actual_items = test_items[test_users == user_idx]\n",
    "        \n",
    "        common_items = len(set(recommendations).intersection(set(actual_items)))\n",
    "        precision = common_items / len(recommendations) if recommendations.size > 0 else 0\n",
    "        recall = common_items / len(actual_items) if len(actual_items) > 0 else 0\n",
    "        \n",
    "        precision_at_k.append(precision)\n",
    "        recall_at_k.append(recall)\n",
    "    \n",
    "    mean_precision = np.mean(precision_at_k)\n",
    "    mean_recall = np.mean(recall_at_k)\n",
    "\n",
    "    print(f\"Skipped {skipped}\") \n",
    "    print(f\"Mean Precision@{k}: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall@{k}: {mean_recall:.4f}\")\n",
    "    \n",
    "    return mean_precision, mean_recall\n",
    "\n",
    "\n",
    "\n",
    "precision, recall = evaluate_recommender_optimized(impressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting time: 92.77 seconds\n",
    "Evaluating users: 100%|██████████| 25538/25538 [01:19<00:00, 320.65it/s]\n",
    "Mean Precision@5: 0.0133\n",
    "Mean Recall@5: 0.0370\n",
    "\n",
    "10_000 rows:\n",
    "Fitting NMF model\n",
    "Model fitting time: 5.65 seconds\n",
    "Evaluating users: 100%|██████████| 1703/1703 [00:05<00:00, 314.24it/s]\n",
    "Mean Precision@5: 0.0011\n",
    "Mean Recall@5: 0.0047\n",
    "\n",
    "100_000 rows:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
